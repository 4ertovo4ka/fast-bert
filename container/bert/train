#!/usr/bin/env python

import os
import json
import pickle
import sys
import traceback
import pandas as pd
import datetime
from pathlib import Path
import logging

import torch
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
from torch.utils.data.distributed import DistributedSampler
from pytorch_pretrained_bert.optimization import BertAdam
from pytorch_pretrained_bert.tokenization import BertTokenizer

from fast_bert.modeling import BertForMultiLabelSequenceClassification
from fast_bert.data import BertDataBunch, InputExample, InputFeatures, MultiLabelTextProcessor, convert_examples_to_features
from fast_bert.learner import BertLearner
from fast_bert.metrics import accuracy, accuracy_multilabel, accuracy_thresh, fbeta, roc_auc

run_start_time = datetime.datetime.today().strftime('%Y-%m-%d_%H-%M-%S')

prefix = '/opt/ml/'

channel_name='training'

input_path = prefix + 'input/data'
finetuned_path = input_path + '/{}/finetuned'.format(channel_name)
output_path = os.path.join(prefix, 'output')
model_path = os.path.join(prefix, 'model')
training_config_path = os.path.join(input_path, '{}/config'.format(channel_name))


PRETRAINED_PATH= Path(os.path.join(prefix, 'code'))
BERT_PRETRAINED_PATH = PRETRAINED_PATH/'pretrained-weights'/'uncased_L-12_H-768_A-12/'

hyperparam_path = os.path.join(prefix, 'input/config/hyperparameters.json')
config_path = os.path.join(training_config_path, 'training_config.json')

# This algorithm has a single channel of input data called 'training'. Since we run in
# File mode, the input files are copied to the directory specified here.

training_path = os.path.join(input_path, channel_name)

def searching_all_files(directory: Path):   
    file_list = [] # A list for storing files existing in directories

    for x in directory.iterdir():
        if x.is_file():
            file_list.append(str(x))
        else:
            file_list.append(searching_all_files(x))

    return file_list

# The function to execute the training.
def train():
        
    print('Starting the training.')
    
    try:
        print(config_path)
        with open(config_path, 'r') as f:
            training_config = json.load(f)
            print(training_config)
            
        with open(hyperparam_path, 'r') as tc:
            hyperparameters = json.load(tc)
            print(hyperparameters)
        
        
        
        # Logger 
        # logfile = str(LOG_PATH/'log-{}-{}.txt'.format(run_start_time, training_config["run_text"]))
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',
            datefmt='%m/%d/%Y %H:%M:%S',
            handlers=[
                #logging.FileHandler(logfile),
                logging.StreamHandler(sys.stdout)
            ])

        logger = logging.getLogger()
        
        finetuned_model_name = training_config.get('finetuned_model', None)
        if finetuned_model_name is not None:
            finetuned_model = os.path.join(finetuned_path, finetuned_model_name)
        else:
            finetuned_model = None
            
        # Tokenizer
        tokenizer = BertTokenizer.from_pretrained(BERT_PRETRAINED_PATH, do_lower_case=bool(training_config['do_lower_case']))
        
        device = torch.device('cuda')
        if torch.cuda.device_count() > 1:
            multi_gpu = True
        else:
            multi_gpu = False
        
        logger.info("Number of GPUs: {}".format(torch.cuda.device_count()))
        
        if multi_gpu == False:
            torch.distributed.init_process_group(backend="nccl", 
                                         init_method = "tcp://localhost:23459", 
                                         rank=0, world_size=1)
        
        if bool(training_config['multi_label']) == True:
            label_col = json.loads(training_config['label_col'])
        else:
            label_col = training_config['label_col']
        logger.info("label columns: {}".format(label_col))
        # Create databunch 
        databunch = BertDataBunch(training_path, training_path, tokenizer, 
                                  train_file=training_config['train_file'], val_file=training_config['val_file'],
                                  label_file=training_config['label_file'],
                                  label_col=label_col
                                  bs=int(hyperparameters['train_batch_size']), maxlen=int(hyperparameters['max_seq_length']),                                                       multi_gpu=multi_gpu, multi_label=bool(training_config['multi_label']))
        
        metrics = []
        if bool(training_config['multi_label']) == False:
            metrics.append({'name': 'accuracy', 'function': accuracy})
        else:
            metrics.append({'name': 'accuracy_thresh', 'function': accuracy_thresh})
            metrics.append({'name': 'roc_auc', 'function': roc_auc})
            metrics.append({'name': 'fbeta', 'function': fbeta})
            metrics.append({'name': 'accuracy_single', 'function': accuracy_multilabel})
        
        logger.info("databunch labels: {}".format(len(databunch.labels)))
        logger.info("multilabel: {}, multilabel type: {}".format(bool(training_config['multi_label']), type(bool(training_config['multi_label']))))
        
        # Initialise the learner
        learner = BertLearner.from_pretrained_model(databunch, BERT_PRETRAINED_PATH, metrics, device, logger, 
                                            finetuned_wgts_path=finetuned_model, 
                                            is_fp16=bool(training_config['fp16']), loss_scale=int(training_config['loss_scale']), 
                                            multi_gpu=multi_gpu,  multi_label=bool(training_config['multi_label']))
        
        learner.fit(int(hyperparameters['epochs']), float(hyperparameters['lr']), schedule_type=hyperparameters['lr_schedule'])
        
        # save model weights
        learner.save_and_reload(model_path, 'pytorch_model')
        
        # save vocab file
        with open(os.path.join(model_path, 'vocab.txt'), "w") as f:
            f.write("\n".join(tokenizer.vocab))
        
        # save model config file
        with open(os.path.join(model_path, 'model_config.json'), "w") as f:
            json.dump(training_config, f)
            
        # save label file
        with open(os.path.join(model_path, 'labels.csv'), "w") as f:
            f.write("\n".join(databunch.labels))
        
        # save model config
        with open(os.path.join(model_path, 'bert_config.json'), "w") as f:
            with open(BERT_PRETRAINED_PATH/'bert_config.json', "r") as rf:
                config = rf.read()
            f.write(config)
    
    except Exception as e:
        # Write out an error file. This will be returned as the failureReason in the
        # DescribeTrainingJob result.
        trc = traceback.format_exc()
        with open(os.path.join(output_path, 'failure'), 'w') as s:
            s.write('Exception during training: ' + str(e) + '\n' + trc)
        # Printing this causes the exception to be in the training job logs, as well.
        print('Exception during training: ' + str(e) + '\n' + trc, file=sys.stderr)
        # A non-zero exit code causes the training job to be marked as Failed.
        sys.exit(255)
        
if __name__ == '__main__':
    train()

    # A zero exit code causes the job to be marked a Succeeded.
    sys.exit(0)
        
        
        
        