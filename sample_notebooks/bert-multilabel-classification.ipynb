{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_pretrained_bert.tokenization import BertTokenizer, WordpieceTokenizer\n",
    "from pytorch_pretrained_bert.modeling import BertForPreTraining, PreTrainedBertModel, BertModel, BertConfig, BertForMaskedLM, BertForSequenceClassification\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import re\n",
    "from torch import Tensor\n",
    "from torch.nn import BCEWithLogitsLoss\n",
    "from fastai.text import Tokenizer, Vocab\n",
    "import pandas as pd\n",
    "import collections\n",
    "import os\n",
    "import pdb\n",
    "from tqdm import tqdm, trange\n",
    "import sys\n",
    "import random\n",
    "import numpy as np\n",
    "import apex\n",
    "from sklearn.model_selection import train_test_split\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from pytorch_pretrained_bert.optimization import BertAdam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH=Path('../data/')\n",
    "DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "PATH=Path('../data/tmp')\n",
    "PATH.mkdir(exist_ok=True)\n",
    "\n",
    "CLAS_DATA_PATH=PATH/'class'\n",
    "CLAS_DATA_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "model_state_dict = None\n",
    "\n",
    "# BERT_PRETRAINED_PATH = Path('../trained_model/')\n",
    "BERT_PRETRAINED_PATH = Path('../models/pretrained-weights/uncased_L-12_H-768_A-12/')\n",
    "# BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/cased_L-12_H-768_A-12/')\n",
    "# BERT_PRETRAINED_PATH = Path('../../complaints/bert/pretrained-weights/uncased_L-24_H-1024_A-16/')\n",
    "\n",
    "\n",
    "# BERT_FINETUNED_WEIGHTS = Path('../trained_model/toxic_comments')\n",
    "\n",
    "PYTORCH_PRETRAINED_BERT_CACHE = BERT_PRETRAINED_PATH/'cache/'\n",
    "PYTORCH_PRETRAINED_BERT_CACHE.mkdir(exist_ok=True)\n",
    "\n",
    "# output_model_file = os.path.join(BERT_FINETUNED_WEIGHTS, \"pytorch_model.bin\")\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "# model_state_dict = torch.load(output_model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    \"train_size\": -1,\n",
    "    \"val_size\": -1,\n",
    "    \"full_data_dir\": DATA_PATH,\n",
    "    \"data_dir\": PATH,\n",
    "    \"task_name\": \"intent_multilabel\",\n",
    "    \"no_cuda\": False,\n",
    "    \"bert_model\": BERT_PRETRAINED_PATH,\n",
    "    \"output_dir\": CLAS_DATA_PATH/'output',\n",
    "    \"max_seq_length\": 512,\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_lower_case\": True,\n",
    "    \"train_batch_size\": 32,\n",
    "    \"eval_batch_size\": 32,\n",
    "    \"learning_rate\": 8e-4,\n",
    "    \"num_train_epochs\": 4.0,\n",
    "    \"warmup_proportion\": 0.1,\n",
    "    \"no_cuda\": False,\n",
    "    \"local_rank\": -1,\n",
    "    \"seed\": 42,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"optimize_on_cpu\": False,\n",
    "    \"fp16\": False,\n",
    "    \"loss_scale\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForMultiLabelSequenceClassification(PreTrainedBertModel):\n",
    "    \"\"\"BERT model for classification.\n",
    "    This module is composed of the BERT model with a linear layer on top of\n",
    "    the pooled output.\n",
    "    Params:\n",
    "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
    "        `num_labels`: the number of classes for the classifier. Default = 2.\n",
    "    Inputs:\n",
    "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
    "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
    "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
    "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
    "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
    "            a `sentence B` token (see BERT paper for more details).\n",
    "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
    "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
    "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
    "            a batch has varying length sentences.\n",
    "        `labels`: labels for the classification output: torch.LongTensor of shape [batch_size]\n",
    "            with indices selected in [0, ..., num_labels].\n",
    "    Outputs:\n",
    "        if `labels` is not `None`:\n",
    "            Outputs the CrossEntropy classification loss of the output with the labels.\n",
    "        if `labels` is `None`:\n",
    "            Outputs the classification logits of shape [batch_size, num_labels].\n",
    "    Example usage:\n",
    "    ```python\n",
    "    # Already been converted into WordPiece token ids\n",
    "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
    "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
    "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
    "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
    "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
    "    num_labels = 2\n",
    "    model = BertForSequenceClassification(config, num_labels)\n",
    "    logits = model(input_ids, token_type_ids, input_mask)\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self, config, num_labels=2):\n",
    "        super(BertForMultiLabelSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = torch.nn.Linear(config.hidden_size, num_labels)\n",
    "        self.apply(self.init_bert_weights)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, labels=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss_fct = BCEWithLogitsLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "        \n",
    "    def freeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def unfreeze_bert_encoder(self):\n",
    "        for param in self.bert.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data representation class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"A single training/test example for simple sequence classification.\"\"\"\n",
    "\n",
    "    def __init__(self, guid, text_a, text_b=None, labels=None):\n",
    "        \"\"\"Constructs a InputExample.\n",
    "\n",
    "        Args:\n",
    "            guid: Unique id for the example.\n",
    "            text_a: string. The untokenized text of the first sequence. For single\n",
    "            sequence tasks, only this sequence must be specified.\n",
    "            text_b: (Optional) string. The untokenized text of the second sequence.\n",
    "            Only must be specified for sequence pair tasks.\n",
    "            labels: (Optional) [string]. The label of the example. This should be\n",
    "            specified for train and dev examples, but not for test examples.\n",
    "        \"\"\"\n",
    "        self.guid = guid\n",
    "        self.text_a = text_a\n",
    "        self.text_b = text_b\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor(object):\n",
    "    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n",
    "\n",
    "    def get_train_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def get_dev_examples(self, data_dir):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n",
    "        raise NotImplementedError() \n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"Gets the list of labels for this data set.\"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "class MultiLabelTextProcessor(DataProcessor):\n",
    "    \n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.labels = None\n",
    "    \n",
    "    \n",
    "    def get_train_examples(self, data_dir, size=-1):\n",
    "        filename = 'train.csv'\n",
    "        logger.info(\"LOOKING AT {}\".format(os.path.join(data_dir, filename)))\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"train\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"train\")\n",
    "        \n",
    "    def get_dev_examples(self, data_dir, size=-1):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        filename = 'val.csv'\n",
    "        if size == -1:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df, \"dev\")\n",
    "        else:\n",
    "            data_df = pd.read_csv(os.path.join(data_dir, filename))\n",
    "#             data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "            return self._create_examples(data_df.sample(size), \"dev\")\n",
    "    \n",
    "    def get_test_examples(self, data_dir, data_file_name, size=-1):\n",
    "        data_df = pd.read_csv(os.path.join(data_dir, data_file_name))\n",
    "#         data_df['comment_text'] = data_df['comment_text'].apply(cleanHtml)\n",
    "        if size == -1:\n",
    "            return self._create_examples(data_df, \"test\")\n",
    "        else:\n",
    "            return self._create_examples(data_df.sample(size), \"test\")\n",
    "\n",
    "    def get_labels(self):\n",
    "        \"\"\"See base class.\"\"\"\n",
    "        if self.labels == None:\n",
    "            self.labels = list(pd.read_csv(os.path.join(self.data_dir, \"labels.csv\"),header=None)[0].values)\n",
    "        return self.labels\n",
    "    \n",
    "    def _get_labels(self, row, label_col):\n",
    "\n",
    "        # create one hot vector of labels\n",
    "        label_list = self.get_labels()\n",
    "        labels = [0] * len(label_list)\n",
    "        labels[label_list.index(row[label_col])] = 1\n",
    "        return labels\n",
    "            \n",
    "    def _create_examples(self, df, set_type, labels_available=True):\n",
    "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
    "        \n",
    "        examples = []\n",
    "        for (i, row) in enumerate(df.values):\n",
    "#             pdb.set_trace()\n",
    "            guid = \"{}-{}\".format(set_type, i)\n",
    "            text_a = row[0]\n",
    "            if labels_available:\n",
    "                labels = self._get_labels(row, 1)\n",
    "            else:\n",
    "                labels = []\n",
    "            examples.append(\n",
    "                InputExample(guid=guid, text_a=text_a, labels=labels))\n",
    "        return examples\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(examples, label_list, max_seq_length, tokenizer):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "\n",
    "    label_map = {label : i for i, label in enumerate(label_list)}\n",
    "\n",
    "    features = []\n",
    "    for (ex_index, example) in enumerate(examples):\n",
    "        tokens_a = tokenizer.tokenize(example.text_a)\n",
    "\n",
    "        tokens_b = None\n",
    "        if example.text_b:\n",
    "            tokens_b = tokenizer.tokenize(example.text_b)\n",
    "            # Modifies `tokens_a` and `tokens_b` in place so that the total\n",
    "            # length is less than the specified length.\n",
    "            # Account for [CLS], [SEP], [SEP] with \"- 3\"\n",
    "            _truncate_seq_pair(tokens_a, tokens_b, max_seq_length - 3)\n",
    "        else:\n",
    "            # Account for [CLS] and [SEP] with \"- 2\"\n",
    "            if len(tokens_a) > max_seq_length - 2:\n",
    "                tokens_a = tokens_a[:(max_seq_length - 2)]\n",
    "\n",
    "        # The convention in BERT is:\n",
    "        # (a) For sequence pairs:\n",
    "        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "        #  type_ids: 0   0  0    0    0     0       0 0    1  1  1  1   1 1\n",
    "        # (b) For single sequences:\n",
    "        #  tokens:   [CLS] the dog is hairy . [SEP]\n",
    "        #  type_ids: 0   0   0   0  0     0 0\n",
    "        #\n",
    "        # Where \"type_ids\" are used to indicate whether this is the first\n",
    "        # sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        # `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        # embedding vector (and position vector). This is not *strictly* necessary\n",
    "        # since the [SEP] token unambigiously separates the sequences, but it makes\n",
    "        # it easier for the model to learn the concept of sequences.\n",
    "        #\n",
    "        # For classification tasks, the first vector (corresponding to [CLS]) is\n",
    "        # used as as the \"sentence vector\". Note that this only makes sense because\n",
    "        # the entire model is fine-tuned.\n",
    "        tokens = [\"[CLS]\"] + tokens_a + [\"[SEP]\"]\n",
    "        segment_ids = [0] * len(tokens)\n",
    "\n",
    "        if tokens_b:\n",
    "            tokens += tokens_b + [\"[SEP]\"]\n",
    "            segment_ids += [1] * (len(tokens_b) + 1)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding = [0] * (max_seq_length - len(input_ids))\n",
    "        input_ids += padding\n",
    "        input_mask += padding\n",
    "        segment_ids += padding\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        \n",
    "        labels_ids = []\n",
    "        for label in example.labels:\n",
    "            labels_ids.append(float(label))\n",
    "\n",
    "#         label_id = label_map[example.label]\n",
    "        if ex_index < 0:\n",
    "            logger.info(\"*** Example ***\")\n",
    "            logger.info(\"guid: %s\" % (example.guid))\n",
    "            logger.info(\"tokens: %s\" % \" \".join(\n",
    "                    [str(x) for x in tokens]))\n",
    "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "            logger.info(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "            logger.info(\n",
    "                    \"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "            logger.info(\"label: %s (id = %s)\" % (example.labels, labels_ids))\n",
    "\n",
    "        features.append(\n",
    "                InputFeatures(input_ids=input_ids,\n",
    "                              input_mask=input_mask,\n",
    "                              segment_ids=segment_ids,\n",
    "                              label_ids=labels_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate_seq_pair(tokens_a, tokens_b, max_length):\n",
    "    \"\"\"Truncates a sequence pair in place to the maximum length.\"\"\"\n",
    "\n",
    "    # This is a simple heuristic which will always truncate the longer sequence\n",
    "    # one token at a time. This makes more sense than truncating an equal percent\n",
    "    # of tokens from each, since if one sequence is very short then each token\n",
    "    # that's truncated likely contains more information than a longer sequence.\n",
    "    while True:\n",
    "        total_length = len(tokens_a) + len(tokens_b)\n",
    "        if total_length <= max_length:\n",
    "            break\n",
    "        if len(tokens_a) > len(tokens_b):\n",
    "            tokens_a.pop()\n",
    "        else:\n",
    "            tokens_b.pop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)\n",
    "\n",
    "def accuracy_thresh(y_pred:Tensor, y_true:Tensor, thresh:float=0.5, sigmoid:bool=True):\n",
    "    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "#     return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n",
    "    return np.mean(((y_pred>thresh)==y_true.byte()).float().cpu().numpy(), axis=1).sum()\n",
    "\n",
    "\n",
    "def fbeta(y_pred:Tensor, y_true:Tensor, thresh:float=0.2, beta:float=2, eps:float=1e-9, sigmoid:bool=True):\n",
    "    \"Computes the f_beta between `preds` and `targets`\"\n",
    "    beta2 = beta ** 2\n",
    "    if sigmoid: y_pred = y_pred.sigmoid()\n",
    "    y_pred = (y_pred>thresh).float()\n",
    "    y_true = y_true.float()\n",
    "    TP = (y_pred*y_true).sum(dim=1)\n",
    "    prec = TP/(y_pred.sum(dim=1)+eps)\n",
    "    rec = TP/(y_true.sum(dim=1)+eps)\n",
    "    res = (prec*rec)/(prec*beta2+rec+eps)*(1+beta2)\n",
    "    return res.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training warmup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def warmup_linear(x, warmup=0.002):\n",
    "    if x < warmup:\n",
    "        return x/warmup\n",
    "    return 1.0 - x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:07:54 - INFO - __main__ -   device: cuda n_gpu: 4, distributed training: False, 16-bits training: False\n"
     ]
    }
   ],
   "source": [
    "processors = {\n",
    "    \"intent_multilabel\": MultiLabelTextProcessor\n",
    "}\n",
    "\n",
    "# Setup GPU parameters\n",
    "\n",
    "if args[\"local_rank\"] == -1 or args[\"no_cuda\"]:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args[\"no_cuda\"] else \"cpu\")\n",
    "    n_gpu = torch.cuda.device_count()\n",
    "#     n_gpu = 1\n",
    "else:\n",
    "    torch.cuda.set_device(args['local_rank'])\n",
    "    device = torch.device(\"cuda\", args['local_rank'])\n",
    "    n_gpu = 1\n",
    "    # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n",
    "    torch.distributed.init_process_group(backend='nccl')\n",
    "logger.info(\"device: {} n_gpu: {}, distributed training: {}, 16-bits training: {}\".format(\n",
    "        device, n_gpu, bool(args['local_rank'] != -1), args['fp16']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['train_batch_size'] = int(args['train_batch_size'] / args['gradient_accumulation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(args['seed'])\n",
    "np.random.seed(args['seed'])\n",
    "torch.manual_seed(args['seed'])\n",
    "if n_gpu > 0:\n",
    "    torch.cuda.manual_seed_all(args['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_name = args['task_name'].lower()\n",
    "\n",
    "if task_name not in processors:\n",
    "    raise ValueError(\"Task not found: %s\" % (task_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = processors[task_name](args['data_dir'])\n",
    "label_list = processor.get_labels()\n",
    "num_labels = len(label_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['enquire-bill',\n",
       "  'problem-hot_water',\n",
       "  'enquire-agreement',\n",
       "  'enquire-safety_certificate',\n",
       "  'enquire-appointment_when',\n",
       "  'enquire-heating',\n",
       "  'request-card',\n",
       "  'enquire-vague',\n",
       "  'enquire-tariff',\n",
       "  'vague-appliance',\n",
       "  'enquire-complaint',\n",
       "  'request-install_meter',\n",
       "  'request-invoice',\n",
       "  'delete-delete',\n",
       "  'enquire-install_boiler',\n",
       "  'renew-agreement',\n",
       "  'None',\n",
       "  'problem-boiler',\n",
       "  'enquire-appointment_plumbing',\n",
       "  'request-annual_service',\n",
       "  'change-address',\n",
       "  'enquire-repair',\n",
       "  'enquire-direct_debit',\n",
       "  'enquire-energy',\n",
       "  'enquire-card',\n",
       "  'enquire-appointment_today',\n",
       "  'report-leak_heating',\n",
       "  'report-home_move',\n",
       "  'change-name',\n",
       "  'setup-direct_debit',\n",
       "  'enquire-balance',\n",
       "  'enquire-bill_energy',\n",
       "  'request-appointment',\n",
       "  'enquire-customer_number',\n",
       "  'pay-bill_energy',\n",
       "  'problem-gas',\n",
       "  'enquire-connection',\n",
       "  'request-smart_meter',\n",
       "  'problem-heating',\n",
       "  'report-appointment_missed',\n",
       "  'vague-plumbing',\n",
       "  'report-letter',\n",
       "  'cancel-contract',\n",
       "  'enquire-refund',\n",
       "  'enquire-payment',\n",
       "  'problem-appliance',\n",
       "  'pay-bill',\n",
       "  'problem-bill',\n",
       "  'problem-vague',\n",
       "  'report-meter_reading_wrong',\n",
       "  'enquire-security_deposit',\n",
       "  'problem-meter_payge',\n",
       "  'change-appointment_service',\n",
       "  'enquire-gas_safety',\n",
       "  'report-contact',\n",
       "  'enquire-price',\n",
       "  'request-appointment_boiler',\n",
       "  'enquire-smart_meter',\n",
       "  'report-leak_water',\n",
       "  'request-service',\n",
       "  'report-meter_reading_final',\n",
       "  'report-bereavement',\n",
       "  'enquire-meter',\n",
       "  'report-direct_debit_unpaid',\n",
       "  'enquire-statement',\n",
       "  'problem-account',\n",
       "  'pay-payment',\n",
       "  'problem-plumbing',\n",
       "  'change-appointment',\n",
       "  'report-meter_reading',\n",
       "  'problem-electric',\n",
       "  'request-advisor',\n",
       "  'problem-meter',\n",
       "  'enquire-annual_service',\n",
       "  'enquire-account',\n",
       "  'request-appointment_electric',\n",
       "  'report-bill_high',\n",
       "  'problem-smart_meter',\n",
       "  'report-leak',\n",
       "  'enquire-meter_exchange',\n",
       "  'change-supplier',\n",
       "  'enquire-quote',\n",
       "  'enquire-appointment',\n",
       "  'register-card',\n",
       "  'problem-card',\n",
       "  'setup-account',\n",
       "  'problem-toilet',\n",
       "  'setup-contract',\n",
       "  'request-bill',\n",
       "  'request-install_meter_payge',\n",
       "  'enquire-payment_plan',\n",
       "  'enquire-meter_payge',\n",
       "  'setup-payment_plan',\n",
       "  'cancel-energy',\n",
       "  'enquire-meter_reading',\n",
       "  'change-account_detail',\n",
       "  'enquire-meter_number',\n",
       "  'enquire-account_online',\n",
       "  'enquire-insurance',\n",
       "  'problem-meter_reading',\n",
       "  'enquire-charge',\n",
       "  'join-homecare',\n",
       "  'problem-boiler_pressure',\n",
       "  'change-direct_debit',\n",
       "  'problem-direct_debit',\n",
       "  'enquire-bill_final',\n",
       "  'problem-key',\n",
       "  'cancel-vague',\n",
       "  'report-bill_unpaid',\n",
       "  'enquire-bill_paid',\n",
       "  'enquire-key',\n",
       "  'enquire-gas_fire',\n",
       "  'join-energy',\n",
       "  'enquire-debt',\n",
       "  'problem-drain',\n",
       "  'enquire-install_meter',\n",
       "  'report-direct_debit_changed',\n",
       "  'report-emergency',\n",
       "  'change-homecare',\n",
       "  'report-no_energy',\n",
       "  'enquire-service',\n",
       "  'enquire-credit',\n",
       "  'cancel-appointment',\n",
       "  'request-quote_energy',\n",
       "  'enquire-nectar',\n",
       "  'request-appointment_plumbing',\n",
       "  'report-bill_paid',\n",
       "  'report-property_new',\n",
       "  'change-payment',\n",
       "  'enquire-hive_hub',\n",
       "  'problem-payment',\n",
       "  'request-meter_move',\n",
       "  'enquire-supplier',\n",
       "  'enquire-invoice',\n",
       "  'enquire-charge_late_payment',\n",
       "  'enquire-account_confirm',\n",
       "  'enquire-warm_home_discount',\n",
       "  'request-meter_remove',\n",
       "  'pay-bill_final',\n",
       "  'vague-top_up',\n",
       "  'problem-hive_hub',\n",
       "  'enquire-debt_meter',\n",
       "  'enquire-meter_new',\n",
       "  'enquire-power_flush',\n",
       "  'enquire-install_smart_meter',\n",
       "  'report-gas_smell',\n",
       "  'request-appointment_meter_reading',\n",
       "  'enquire-call_back',\n",
       "  'enquire-meter_reset',\n",
       "  'vague-breakdown',\n",
       "  'report-card_lost',\n",
       "  'enquire-gas_capped',\n",
       "  'report-cut_off',\n",
       "  'request-service_report',\n",
       "  'problem-home_move',\n",
       "  'report-payment_difficulty',\n",
       "  'enquire-bulb',\n",
       "  'request-balance',\n",
       "  'report-letter_leaving',\n",
       "  'enquire-heating_upgrade',\n",
       "  'enquire-warranty',\n",
       "  'cancel-direct_debit',\n",
       "  'enquire-hive_heating',\n",
       "  'request-appointment_gas',\n",
       "  'enquire-bill_due',\n",
       "  'problem-pipe',\n",
       "  'enquire-bill_homecare',\n",
       "  'dispute-bill',\n",
       "  'change-bill',\n",
       "  'enquire-usage',\n",
       "  'pay-homecare',\n",
       "  'enquire-install_heating',\n",
       "  'problem-water',\n",
       "  'change-phone',\n",
       "  'request-appointment_emergency',\n",
       "  'change-email',\n",
       "  'change-vague',\n",
       "  'problem-shower',\n",
       "  'report-business_move',\n",
       "  'enquire-power_cut',\n",
       "  'request-bill_paper',\n",
       "  'enquire-gas_pipe',\n",
       "  'request-dyno_rod',\n",
       "  'report-key_lost',\n",
       "  'report-leak_drain',\n",
       "  'report-house_sale',\n",
       "  'report-blockage',\n",
       "  'request-appointment_homecare',\n",
       "  'request-appointment_heating',\n",
       "  'enquire-appointment_smart_meter',\n",
       "  'enquire-password',\n",
       "  'enquire-business_account',\n",
       "  'change-appointment_boiler',\n",
       "  'problem-bill_energy',\n",
       "  'vague-business',\n",
       "  'enquire-activation_code',\n",
       "  'enquire-boiler_new',\n",
       "  'vague-landlord',\n",
       "  'enquire-appointment_boiler',\n",
       "  'enquire-boiler_iq',\n",
       "  'report-leak_gas',\n",
       "  'change-install_smart_meter',\n",
       "  'vague-solar_panel',\n",
       "  'enquire-authority',\n",
       "  'enquire-bill_estimate',\n",
       "  'enquire-voucher',\n",
       "  'enquire-bill_reminder',\n",
       "  'enquire-hive',\n",
       "  'vague-pest_control',\n",
       "  'vague-fraud',\n",
       "  'enquire-emergency_credit',\n",
       "  'request-locksmith',\n",
       "  'report-meter_reading_opening',\n",
       "  'cancel-tariff',\n",
       "  'enquire-smart_card',\n",
       "  'cancel-smart_meter',\n",
       "  'enquire-claim',\n",
       "  'cancel-business_account',\n",
       "  'vague-compensation',\n",
       "  'problem-hive_app',\n",
       "  'vague-carbon_monoxide',\n",
       "  'enquire-account_overdue',\n",
       "  'change-payment_plan',\n",
       "  'problem-hive',\n",
       "  'enquire-install',\n",
       "  'enquire-fuel_direct',\n",
       "  'enquire-alarm',\n",
       "  'enquire-order',\n",
       "  'problem-hive_heating',\n",
       "  'setup-hive',\n",
       "  'report-gas_emergency',\n",
       "  'report-leak_sink',\n",
       "  'enquire-repayment',\n",
       "  'change-ownership',\n",
       "  'enquire-property',\n",
       "  'enquire-insulation',\n",
       "  'problem-payment_plan',\n",
       "  'report-payment_late',\n",
       "  'problem-account_setup',\n",
       "  'enquire-hive_camera',\n",
       "  'enquire-hive_sensor',\n",
       "  'enquire-hive_bulb',\n",
       "  'enquire-hive_thermostat',\n",
       "  'enquire-hive_app',\n",
       "  'enquire-install_hive',\n",
       "  'problem-hive_thermostat',\n",
       "  'cancel-hive',\n",
       "  'problem-hive_sensor',\n",
       "  'problem-hive_camera',\n",
       "  'enquire-business_care'],\n",
       " 250)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list, num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:07:54 - INFO - pytorch_pretrained_bert.tokenization -   loading vocabulary file ../models/pretrained-weights/uncased_L-12_H-768_A-12/vocab.txt\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args['bert_model'], do_lower_case=args['do_lower_case'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:07:54 - INFO - __main__ -   LOOKING AT ../data/train.csv\n"
     ]
    }
   ],
   "source": [
    "train_examples = None\n",
    "num_train_steps = None\n",
    "if args['do_train']:\n",
    "    train_examples = processor.get_train_examples(args['full_data_dir'], size=args['train_size'])\n",
    "#     train_examples = processor.get_train_examples(args['data_dir'], size=args['train_size'])\n",
    "    num_train_steps = int(\n",
    "        len(train_examples) / args['train_batch_size'] / args['gradient_accumulation_steps'] * args['num_train_epochs'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:07:54 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ../models/pretrained-weights/uncased_L-12_H-768_A-12 from cache at ../models/pretrained-weights/uncased_L-12_H-768_A-12\n",
      "02/11/2019 00:07:54 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "02/11/2019 00:08:00 - INFO - pytorch_pretrained_bert.modeling -   Weights of BertForMultiLabelSequenceClassification not initialized from pretrained model: ['classifier.weight', 'classifier.bias']\n",
      "02/11/2019 00:08:00 - INFO - pytorch_pretrained_bert.modeling -   Weights from pretrained model not used in BertForMultiLabelSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
     ]
    }
   ],
   "source": [
    "# Prepare model\n",
    "def get_model():\n",
    "#     pdb.set_trace()\n",
    "    if model_state_dict:\n",
    "        model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "    else:\n",
    "        model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels)\n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "if args['fp16']:\n",
    "    model.half()\n",
    "model.to(device)\n",
    "if args['local_rank'] != -1:\n",
    "    try:\n",
    "        from apex.parallel import DistributedDataParallel as DDP\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    model = DDP(model)\n",
    "elif n_gpu > 1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import _LRScheduler, Optimizer\n",
    "\n",
    "class CyclicLR(object):\n",
    "    \"\"\"Sets the learning rate of each parameter group according to\n",
    "    cyclical learning rate policy (CLR). The policy cycles the learning\n",
    "    rate between two boundaries with a constant frequency, as detailed in\n",
    "    the paper `Cyclical Learning Rates for Training Neural Networks`_.\n",
    "    The distance between the two boundaries can be scaled on a per-iteration\n",
    "    or per-cycle basis.\n",
    "    Cyclical learning rate policy changes the learning rate after every batch.\n",
    "    `batch_step` should be called after a batch has been used for training.\n",
    "    To resume training, save `last_batch_iteration` and use it to instantiate `CycleLR`.\n",
    "    This class has three built-in policies, as put forth in the paper:\n",
    "    \"triangular\":\n",
    "        A basic triangular cycle w/ no amplitude scaling.\n",
    "    \"triangular2\":\n",
    "        A basic triangular cycle that scales initial amplitude by half each cycle.\n",
    "    \"exp_range\":\n",
    "        A cycle that scales initial amplitude by gamma**(cycle iterations) at each\n",
    "        cycle iteration.\n",
    "    This implementation was adapted from the github repo: `bckenstler/CLR`_\n",
    "    Args:\n",
    "        optimizer (Optimizer): Wrapped optimizer.\n",
    "        base_lr (float or list): Initial learning rate which is the\n",
    "            lower boundary in the cycle for eachparam groups.\n",
    "            Default: 0.001\n",
    "        max_lr (float or list): Upper boundaries in the cycle for\n",
    "            each parameter group. Functionally,\n",
    "            it defines the cycle amplitude (max_lr - base_lr).\n",
    "            The lr at any cycle is the sum of base_lr\n",
    "            and some scaling of the amplitude; therefore\n",
    "            max_lr may not actually be reached depending on\n",
    "            scaling function. Default: 0.006\n",
    "        step_size (int): Number of training iterations per\n",
    "            half cycle. Authors suggest setting step_size\n",
    "            2-8 x training iterations in epoch. Default: 2000\n",
    "        mode (str): One of {triangular, triangular2, exp_range}.\n",
    "            Values correspond to policies detailed above.\n",
    "            If scale_fn is not None, this argument is ignored.\n",
    "            Default: 'triangular'\n",
    "        gamma (float): Constant in 'exp_range' scaling function:\n",
    "            gamma**(cycle iterations)\n",
    "            Default: 1.0\n",
    "        scale_fn (function): Custom scaling policy defined by a single\n",
    "            argument lambda function, where\n",
    "            0 <= scale_fn(x) <= 1 for all x >= 0.\n",
    "            mode paramater is ignored\n",
    "            Default: None\n",
    "        scale_mode (str): {'cycle', 'iterations'}.\n",
    "            Defines whether scale_fn is evaluated on\n",
    "            cycle number or cycle iterations (training\n",
    "            iterations since start of cycle).\n",
    "            Default: 'cycle'\n",
    "        last_batch_iteration (int): The index of the last batch. Default: -1\n",
    "    Example:\n",
    "        >>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "        >>> scheduler = torch.optim.CyclicLR(optimizer)\n",
    "        >>> data_loader = torch.utils.data.DataLoader(...)\n",
    "        >>> for epoch in range(10):\n",
    "        >>>     for batch in data_loader:\n",
    "        >>>         scheduler.batch_step()\n",
    "        >>>         train_batch(...)\n",
    "    .. _Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    .. _bckenstler/CLR: https://github.com/bckenstler/CLR\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "#         if not isinstance(optimizer, Optimizer):\n",
    "#             raise TypeError('{} is not an Optimizer'.format(\n",
    "#                 type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "    ]\n",
    "t_total = num_train_steps\n",
    "if args['local_rank'] != -1:\n",
    "    t_total = t_total // torch.distributed.get_world_size()\n",
    "if args['fp16']:\n",
    "    try:\n",
    "        from apex.optimizers import FP16_Optimizer\n",
    "        from apex.optimizers import FusedAdam\n",
    "    except ImportError:\n",
    "        raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use distributed and fp16 training.\")\n",
    "\n",
    "    optimizer = FusedAdam(optimizer_grouped_parameters,\n",
    "                          lr=args['learning_rate'],\n",
    "                          bias_correction=False,\n",
    "                          max_grad_norm=1.0)\n",
    "    if args['loss_scale'] == 0:\n",
    "        optimizer = FP16_Optimizer(optimizer, dynamic_loss_scale=True)\n",
    "    else:\n",
    "        optimizer = FP16_Optimizer(optimizer, static_loss_scale=args['loss_scale'])\n",
    "\n",
    "else:\n",
    "    optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                         lr=args['learning_rate'],\n",
    "                         warmup=args['warmup_proportion'],\n",
    "                         t_total=t_total)\n",
    "\n",
    "scheduler = CyclicLR(optimizer, base_lr=2e-5, max_lr=5e-5, step_size=2500, last_batch_iteration=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eval Fn\n",
    "eval_examples = processor.get_dev_examples(args['data_dir'], size=args['val_size'])\n",
    "def eval():\n",
    "    args['output_dir'].mkdir(exist_ok=True)\n",
    "\n",
    "    \n",
    "    eval_features = convert_examples_to_features(\n",
    "        eval_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    logger.info(\"***** Running evaluation *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(eval_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in eval_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in eval_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in eval_features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in eval_features], dtype=torch.float)\n",
    "    eval_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    # Run prediction for full data\n",
    "    eval_sampler = SequentialSampler(eval_data)\n",
    "    eval_dataloader = DataLoader(eval_data, sampler=eval_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    all_labels = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for input_ids, input_mask, segment_ids, label_ids in eval_dataloader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "        label_ids = label_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            tmp_eval_loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "\n",
    "#         logits = logits.detach().cpu().numpy()\n",
    "#         label_ids = label_ids.to('cpu').numpy()\n",
    "#         tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "        tmp_eval_accuracy = accuracy_thresh(logits, label_ids)\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        if all_labels is None:\n",
    "            all_labels = label_ids.detach().cpu().numpy()\n",
    "        else:    \n",
    "            all_labels = np.concatenate((all_labels, label_ids.detach().cpu().numpy()), axis=0)\n",
    "        \n",
    "\n",
    "        eval_loss += tmp_eval_loss.mean().item()\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "\n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    eval_accuracy = eval_accuracy / nb_eval_examples\n",
    "    \n",
    "#     ROC-AUC calcualation\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    \n",
    "    for i in range(num_labels):\n",
    "        fpr[i], tpr[i], _ = roc_curve(all_labels[:, i], all_logits[:, i])\n",
    "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "        \n",
    "    # Compute micro-average ROC curve and ROC area\n",
    "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(all_labels.ravel(), all_logits.ravel())\n",
    "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
    "\n",
    "    result = {'eval_loss': eval_loss,\n",
    "              'eval_accuracy': eval_accuracy,\n",
    "#               'loss': tr_loss/nb_tr_steps,\n",
    "              'roc_auc': roc_auc  }\n",
    "\n",
    "    output_eval_file = os.path.join(args['output_dir'], \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        logger.info(\"***** Eval results *****\")\n",
    "        for key in sorted(result.keys()):\n",
    "            logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "#             writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = convert_examples_to_features(\n",
    "    train_examples, label_list, args['max_seq_length'], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:08:09 - INFO - __main__ -   ***** Running training *****\n",
      "02/11/2019 00:08:09 - INFO - __main__ -     Num examples = 16220\n",
      "02/11/2019 00:08:09 - INFO - __main__ -     Batch size = 32\n",
      "02/11/2019 00:08:09 - INFO - __main__ -     Num steps = 2027\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"***** Running training *****\")\n",
    "logger.info(\"  Num examples = %d\", len(train_examples))\n",
    "logger.info(\"  Batch size = %d\", args['train_batch_size'])\n",
    "logger.info(\"  Num steps = %d\", num_train_steps)\n",
    "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
    "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
    "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
    "all_label_ids = torch.tensor([f.label_ids for f in train_features], dtype=torch.float)\n",
    "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "if args['local_rank'] == -1:\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "else:\n",
    "    train_sampler = DistributedSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=args['train_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(num_epocs=args['num_train_epochs']):\n",
    "    global_step = 0\n",
    "    model.train()\n",
    "    for i_ in tqdm(range(int(num_epocs)), desc=\"Epoch\"):\n",
    "\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "        for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
    "\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            input_ids, input_mask, segment_ids, label_ids = batch\n",
    "            loss = model(input_ids, segment_ids, input_mask, label_ids)\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean() # mean() to average on multi-gpu.\n",
    "            if args['gradient_accumulation_steps'] > 1:\n",
    "                loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "            if args['fp16']:\n",
    "                optimizer.backward(loss)\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "    #             scheduler.batch_step()\n",
    "                # modify learning rate with special warm up BERT uses\n",
    "                lr_this_step = args['learning_rate'] * warmup_linear(global_step/t_total, args['warmup_proportion'])\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = lr_this_step\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "        logger.info('Loss after epoc {}'.format(tr_loss / nb_tr_steps))\n",
    "        logger.info('Eval after epoc {}'.format(i_+1))\n",
    "        eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze BERT layers for 1 epoch\n",
    "# model.module.freeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.module.unfreeze_bert_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ae0cd6963534513aa9f25c5d6d0e79a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4, style=ProgressStyle(description_width='initial…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0426082ddda6470c91cfaaa01e9428a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=507, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "02/11/2019 00:11:32 - INFO - __main__ -   Loss after epoc 0.06884018420673452\n",
      "02/11/2019 00:11:32 - INFO - __main__ -   Eval after epoc 1\n",
      "02/11/2019 00:11:33 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2019 00:11:33 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:11:33 - INFO - __main__ -     Batch size = 32\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/ranking.py:547: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "02/11/2019 00:12:02 - INFO - __main__ -   ***** Eval results *****\n",
      "02/11/2019 00:12:02 - INFO - __main__ -     eval_accuracy = 0.9959999921730502\n",
      "02/11/2019 00:12:02 - INFO - __main__ -     eval_loss = 0.02320338011257292\n",
      "02/11/2019 00:12:02 - INFO - __main__ -     roc_auc = {0: 0.6380609357197986, 1: 0.6821573033707865, 2: 0.7162988532158001, 3: 0.6136700836255732, 4: 0.7171017369727046, 5: 0.27104418662058755, 6: 0.8359071363598776, 7: 0.5131222731024724, 8: 0.5950646778535202, 9: 0.3811124845488257, 10: 0.7071232876712328, 11: 0.3750142732082366, 12: 0.6083826916748931, 13: nan, 14: 0.6490123456790123, 15: 0.7111234983802299, 16: nan, 17: 0.779652605459057, 18: 0.2643068574247657, 19: 0.6222633583710084, 20: 0.32847790934223564, 21: 0.5609207805053502, 22: 0.7132373800584063, 23: 0.727983436853002, 24: 0.5673934588701685, 25: 0.34138302277432714, 26: 0.42197306185553507, 27: 0.6556015559083044, 28: 0.7102282878411911, 29: 0.8804211818589607, 30: 0.6265797532781228, 31: 0.5473957431200138, 32: 0.41581257783312575, 33: 0.32979511231794617, 34: 0.3243377073533053, 35: 0.28017283950617283, 36: 0.5016951955676838, 37: 0.5057022158950235, 38: 0.4596995280583491, 39: 0.4418659693889052, 40: 0.020303628733645995, 41: 0.7130639563708477, 42: 0.4717442664296513, 43: 0.7173548089691282, 44: 0.5968961732989789, 45: 0.6455663677595165, 46: 0.6569247924295164, 47: 0.5395657880345619, 48: 0.5413127225442659, 49: 0.7634601136082984, 50: nan, 51: 0.5030578206078576, 52: 0.3067299548277809, 53: 0.7214332757059356, 54: 0.8605990783410137, 55: 0.2430264132313009, 56: 0.5693111236589498, 57: 0.3682803314682343, 58: 0.4133486314396758, 59: 0.7281500106239819, 60: 0.4025476131585456, 61: 0.5538201320132012, 62: 0.5604976745284133, 63: 0.21512010529779535, 64: 0.6370649222414219, 65: 0.5279856013551666, 66: 0.5267616850661834, 67: 0.5176717745921898, 68: 0.524441384005942, 69: 0.5296259103950562, 70: 0.4617278122471, 71: 0.8830626162035504, 72: 0.48650101672103807, 73: 0.6583681818181817, 74: 0.7008214125383709, 75: 0.31432098765432104, 76: 0.5278874629812439, 77: 0.3094337202590906, 78: 0.4030145787002718, 79: 0.4992803970223325, 80: 0.6377351679619484, 81: 0.5849938195302843, 82: 0.5495085416706701, 83: 0.5817390767711677, 84: 0.6693432465923171, 85: 0.4745291201982652, 86: 0.47107806691449816, 87: 0.47585823660163007, 88: 0.5655638166047088, 89: 0.2850222167366082, 90: 0.6164197530864197, 91: 0.3946353522867738, 92: 0.5566526783510244, 93: 0.5128737060041407, 94: 0.5552992186232002, 95: 0.7756172074973164, 96: 0.47983930778739187, 97: 0.8217183655414558, 98: 0.6915905161768338, 99: 0.6643811881188119, 100: 0.33550357936312025, 101: 0.4832717999467134, 102: 0.5351186063750927, 103: 0.661776397515528, 104: 0.8162788778877887, 105: 0.40404393555341894, 106: 0.5182982529593119, 107: 0.34380293619424057, 108: 0.48586000837361554, 109: 0.5656761716544325, 110: 0.297909947903746, 111: 0.6398597359735974, 112: 0.3488467247744833, 113: 0.6615808368408023, 114: 0.6655344763555335, 115: 0.5595954021238534, 116: 0.7971515600559809, 117: 0.29919536984754375, 118: 0.6856353542161836, 119: 0.3754993459893237, 120: 0.48687031082529475, 121: 0.7387832386597514, 122: 0.6978213399503722, 123: 0.3729336294103134, 124: 0.4464444444444444, 125: 0.3810200049394912, 126: 0.4916831560722236, 127: 0.3056103981572886, 128: 0.5475410173963229, 129: 0.10140636565507033, 130: 0.6654794161349532, 131: 0.2811497656057242, 132: 0.6987733596772865, 133: 0.2721042232650037, 134: 0.8357178095707943, 135: 0.8382316621387997, 136: 0.31034091845988954, 137: 0.34559368057269807, 138: 0.43541718094297704, 139: 0.6171004580341668, 140: 0.5279150197628458, 141: 0.7027627035027134, 142: nan, 143: 0.49557503910430567, 144: 0.47419408030340504, 145: 0.3249012833168805, 146: 0.7069454724895139, 147: 0.09209227732543795, 148: nan, 149: 0.29495523699675946, 150: 0.5352112676056338, 151: nan, 152: 0.6702451464297465, 153: 0.9035520473606315, 154: 0.6202286936492267, 155: 0.5087344720496895, 156: 0.2577103380212189, 157: 0.6236296296296295, 158: 0.253037037037037, 159: nan, 160: 0.40241796200345425, 161: 0.8462469135802468, 162: 0.48321816386969396, 163: 0.7436296296296296, 164: 0.809395577208541, 165: 0.46537558685446007, 166: 0.8276750423489554, 167: 0.5696780443565009, 168: 0.6987694781103142, 169: 0.681197003375319, 170: 0.5875092569735868, 171: 0.4890204786577843, 172: 0.44011187890753534, 173: 0.14939550949913644, 174: 0.248766345916605, 175: 0.8643564356435643, 176: 0.41233819770797264, 177: 0.35949205565160125, 178: 0.645324451023933, 179: 0.34989059853190285, 180: 0.828596144340089, 181: nan, 182: 0.26530108588351425, 183: 0.4103688153453528, 184: 0.7359398125308337, 185: 0.5076524314983955, 186: 0.23680315737543167, 187: 0.6909096519377931, 188: 0.48920552677029366, 189: 0.5562469135802469, 190: 0.7772697314219806, 191: 0.7117923362175526, 192: 0.5056022408963585, 193: 0.3816938540121931, 194: 0.07994078460399703, 195: 0.3324697754749568, 196: 0.29326092322883235, 197: 0.753947693066864, 198: 0.6446036058285997, 199: nan, 200: 0.3507895386133728, 201: 0.5542530437643961, 202: 0.575993091537133, 203: nan, 204: 0.6882411067193676, 205: 0.7990619600098741, 206: 0.5309723593287266, 207: 0.38956170703575543, 208: nan, 209: 0.9015786877158362, 210: 0.185002466699556, 211: nan, 212: 0.6564789659998354, 213: 0.9982733103108041, 214: 0.5352738036507154, 215: 0.5947446336047373, 216: 0.3021835677276092, 217: 0.29390725209669466, 218: 0.3755550074000986, 219: 0.9847064627528368, 220: nan, 221: 0.5305870744943266, 222: 0.5853478046373952, 223: 0.6984301606922126, 224: 0.26061466304616143, 225: 0.3097088576363188, 226: 0.501973846533432, 227: 0.212323132609411, 228: 0.41945679012345677, 229: 0.47618460019743336, 230: 0.005920078934385842, 231: nan, 232: 0.45843611248149974, 233: 0.5386640342217834, 234: 0.7887367382186035, 235: nan, 236: 0.10473723168023683, 237: 0.9335224469659595, 238: 0.7705196247840039, 239: 0.1936220083888478, 240: 0.1746423285643809, 241: 0.09472126295017269, 242: nan, 243: 0.67501233349778, 244: 0.43413912185495807, 245: 0.6423703703703704, 246: 0.40663542180562406, 247: 0.4141376757957069, 248: 0.2729459659511473, 249: 0.5069378159578605, 'micro': 0.8152959649295027}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a03706674d0407c9cedf9645fbe6370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=507, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:15:08 - INFO - __main__ -   Loss after epoc 0.02254110870467723\n",
      "02/11/2019 00:15:08 - INFO - __main__ -   Eval after epoc 2\n",
      "02/11/2019 00:15:09 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2019 00:15:09 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:15:09 - INFO - __main__ -     Batch size = 32\n",
      "02/11/2019 00:15:37 - INFO - __main__ -   ***** Eval results *****\n",
      "02/11/2019 00:15:37 - INFO - __main__ -     eval_accuracy = 0.9959999921730502\n",
      "02/11/2019 00:15:37 - INFO - __main__ -     eval_loss = 0.020217545931850833\n",
      "02/11/2019 00:15:37 - INFO - __main__ -     roc_auc = {0: 0.9585706935205263, 1: 0.8820474406991261, 2: 0.9424275339613839, 3: 0.6754338638611636, 4: 0.882560794044665, 5: 0.3093063441125648, 6: 0.9559559249152402, 7: 0.49102334813883103, 8: 0.9615978550010312, 9: 0.3840049443757726, 10: 0.8665753424657535, 11: 0.7626270315533058, 12: 0.5450806186245476, 13: nan, 14: 0.3857283950617284, 15: 0.9766809658838451, 16: nan, 17: 0.9724119106699752, 18: 0.4479526393685249, 19: 0.9803673774518529, 20: 0.662602368160242, 21: 0.4813416059706861, 22: 0.9572131831455987, 23: 0.6169855072463768, 24: 0.9343148505555265, 25: 0.878136645962733, 26: 0.8874716664701335, 27: 0.9748183820269709, 28: 0.9597022332506203, 29: 0.9871095686663361, 30: 0.6534247757073844, 31: 0.9045156754663922, 32: 0.9026961394769615, 33: 0.5643051098494198, 34: 0.8918513245852934, 35: 0.8871111111111111, 36: 0.7632824774663028, 37: 0.9191012626887843, 38: 0.8967582189838347, 39: 0.8788179338638054, 40: 0.8541718094297704, 41: 0.9656963429893525, 42: 0.9878377659776506, 43: 0.9353701244641985, 44: 0.9158435919457963, 45: 0.86146077904444, 46: 0.8943462471557947, 47: 0.8721304701836257, 48: 0.601468032617546, 49: 0.7963694739441838, 50: nan, 51: 0.9548122065727698, 52: 0.7069099378881987, 53: 0.6572610521116325, 54: 0.9031007751937984, 55: 0.38879289064428535, 56: 0.9511575381140598, 57: 0.8898765257218405, 58: 0.8530554866451666, 59: 0.8633519843237245, 60: 0.7217825047407042, 61: 0.6134488448844885, 62: 0.9075182378355408, 63: 0.978035538005923, 64: 0.363367069859294, 65: 0.8048066064370413, 66: 0.7393035645630801, 67: 0.6869885208985556, 68: 0.8721063382025254, 69: 0.9714080776870448, 70: 0.7672196744897042, 71: 0.9525571148409445, 72: 0.8263528241438329, 73: 0.8105454545454545, 74: 0.9360978657833752, 75: 0.829432098765432, 76: 0.4364100032905561, 77: 0.7944350028859103, 78: 0.8891462811959476, 79: 0.8219007444168734, 80: 0.784744341105024, 81: 0.6623485784919654, 82: 0.926708814542939, 83: 0.9120587509256973, 84: 0.9494671623296158, 85: 0.8210780669144983, 86: 0.7470508054522924, 87: 0.716102741417634, 88: 0.6443122676579925, 89: 0.9652554924709948, 90: 0.9314074074074074, 91: 0.953053152039555, 92: 0.788632436435448, 93: 0.8543188405797102, 94: 0.8780113148925317, 95: 0.8216222167175846, 96: 0.8366625463535229, 97: 0.6999391777943658, 98: 0.7363134930435498, 99: 0.7906600660066007, 100: 0.678659590224636, 101: 0.6771971225212194, 102: 0.7400543612552508, 103: 0.977879917184265, 104: 0.9167656765676567, 105: 0.8427293651487341, 106: 0.9604536977124805, 107: 0.9191840767927725, 108: 0.4800936322460321, 109: 0.7786208356860531, 110: 0.9063921276771686, 111: 0.8324587458745875, 112: 0.3219845468732158, 113: 0.851015102748205, 114: 0.8304964100024759, 115: 0.832318349636509, 116: 0.9631596278916605, 117: 0.3630717108977979, 118: 0.8322395260562632, 119: 0.689592392264998, 120: 0.6049550663698574, 121: 0.7525314892566065, 122: 0.8805856079404467, 123: 0.1902294596595115, 124: 0.7504691358024691, 125: 0.9216679015394748, 126: 0.8663739797180312, 127: 0.9086870681145114, 128: 0.726997279248083, 129: 0.6043671354552184, 130: 0.7988083792209306, 131: 0.8275351591413768, 132: 0.39557092286161194, 133: 0.7575944677698198, 134: 0.7918105574740997, 135: 0.5983782003786944, 136: 0.4977739302498145, 137: 0.8559614909898791, 138: 0.8893483090594915, 139: 0.5005106461995543, 140: 0.579792490118577, 141: 0.8852984706462752, 142: nan, 143: 0.5382810570511238, 144: 0.9154505730068431, 145: 0.7316551497203027, 146: 0.731680236861584, 147: 0.8993338267949667, 148: nan, 149: 0.25984511451639475, 150: 0.947677291821102, 151: nan, 152: 0.5644126357354393, 153: 0.28835717809570793, 154: 0.29828891082592957, 155: 0.8344508752117448, 156: 0.5314581791265729, 157: 0.8726913580246913, 158: 0.7858271604938272, 159: nan, 160: 0.30520602023192694, 161: 0.7095308641975309, 162: 0.4258802237578151, 163: 0.8895308641975308, 164: 0.5780839645263197, 165: 0.935137138621201, 166: 0.5304912478825523, 167: 0.5128823480913514, 168: 0.5583518839145849, 169: 0.42792459043385195, 170: 0.6557640088866946, 171: 0.9232667160128301, 172: 0.4813260941099045, 173: 0.5857389587959536, 174: 0.7156427337774488, 175: 0.7443894389438944, 176: 0.713249237364993, 177: 0.6285502593232897, 178: 0.5291142363681224, 179: 0.8307806324110671, 180: 0.1853133410226836, 181: nan, 182: 0.3549687397170122, 183: 0.9792952992508439, 184: 0.6218549580661075, 185: 0.23815107380893605, 186: 0.47089294523926983, 187: 0.4343989138484325, 188: 0.6961509992598076, 189: 0.8898765432098765, 190: 0.7819519964848685, 191: 0.5079110012360939, 192: 0.5908167188444005, 193: 0.5807107156588125, 194: 0.03873673821860352, 195: 0.6838144584258574, 196: 0.47877067390767714, 197: 0.4471996052306933, 198: 0.7804396147196838, 199: nan, 200: 0.9085862324204292, 201: 0.8857354392892399, 202: 0.5556377991611152, 203: nan, 204: 0.5689935064935066, 205: 0.9194026166378672, 206: 0.5120927936821322, 207: 0.5400120832646784, 208: nan, 209: 0.6176615688209176, 210: 0.5271336951159349, 211: nan, 212: 0.47361488433358034, 213: 0.20128268376911695, 214: 0.6492353231376418, 215: 0.997779422649889, 216: 0.7508018751542067, 217: 0.5017266896891959, 218: 0.5310804144055254, 219: 0.5456339417858904, 220: nan, 221: 0.26615688209176125, 222: 0.9148988653182042, 223: 0.6437824474660073, 224: 0.9817946186126881, 225: 0.04379471996052309, 226: 0.9079694053787318, 227: 0.8002632444883185, 228: 0.3304691358024691, 229: 0.7672754195459032, 230: 0.6265416872224963, 231: nan, 232: 0.5273803650715343, 233: 0.456071076011846, 234: 0.2182334073525783, 235: nan, 236: 0.35991857883049594, 237: 0.899605328071041, 238: 0.7804245865218464, 239: 0.8724401677769553, 240: 0.9082387765170201, 241: 0.9595461272816971, 242: nan, 243: 0.9181055747409965, 244: 0.96250616674889, 245: 0.575358024691358, 246: 0.6679822397631969, 247: 0.540340488527017, 248: 0.7902788058228473, 249: 0.8348004383639127, 'micro': 0.8734234827624846}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b4a057f96404ca5a672a415472a0a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=507, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:18:43 - INFO - __main__ -   Loss after epoc 0.01731222462188269\n",
      "02/11/2019 00:18:43 - INFO - __main__ -   Eval after epoc 3\n",
      "02/11/2019 00:18:45 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2019 00:18:45 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:18:45 - INFO - __main__ -     Batch size = 32\n",
      "02/11/2019 00:19:13 - INFO - __main__ -   ***** Eval results *****\n",
      "02/11/2019 00:19:13 - INFO - __main__ -     eval_accuracy = 0.9966490861960904\n",
      "02/11/2019 00:19:13 - INFO - __main__ -     eval_loss = 0.014837246813525365\n",
      "02/11/2019 00:19:13 - INFO - __main__ -     roc_auc = {0: 0.988969642397736, 1: 0.976808988764045, 2: 0.9823874278472905, 3: 0.9610871324521175, 4: 0.9564962779156326, 5: 0.9694519871636633, 6: 0.9928471016290417, 7: 0.9310242325380292, 8: 0.9857727231861908, 9: 0.9262051915945612, 10: 0.9686114570361145, 11: 0.9220873139725194, 12: 0.8904244817374136, 13: nan, 14: 0.8905185185185184, 15: 0.9985821962628147, 16: nan, 17: 0.9916129032258064, 18: 0.9023186975826344, 19: 0.9842179583021846, 20: 0.9217437796672057, 21: 0.7260138476755688, 22: 0.9897830621610346, 23: 0.8979792960662526, 24: 0.9928798706379428, 25: 0.9725879917184265, 26: 0.9963444833143351, 27: 0.98710440888355, 28: 0.9819454094292803, 29: 0.986117997025285, 30: 0.788313060731539, 31: 0.981751862002914, 32: 0.9573661270236613, 33: 0.6466921747716613, 34: 0.990963109680614, 35: 0.9557037037037036, 36: 0.9761949061440502, 37: 0.9712954939341422, 38: 0.9915028687362291, 39: 0.8616752699321506, 40: 0.8931128116514441, 41: 0.9651179261987393, 42: 0.9966992562881257, 43: 0.9920395203071356, 44: 0.9789459872125204, 45: 0.9965111783730776, 46: 0.9942739176951012, 47: 0.9352692405847826, 48: 0.9027063813239029, 49: 0.969375154359101, 50: nan, 51: 0.9853286384976525, 52: 0.8423560135516657, 53: 0.8877500617436405, 54: 0.9987764798342442, 55: 0.5530733152308072, 56: 0.8294042913608131, 57: 0.9767688929810893, 58: 0.9986769205325394, 59: 0.9764029558278443, 60: 0.9032071893808227, 61: 0.8068481848184819, 62: 0.964740184621196, 63: 0.9782823297137216, 64: 0.7393853369538386, 65: 0.9234895539243365, 66: 0.8777667929917065, 67: 0.9847860712912617, 68: 0.9116272592225798, 69: 0.9897660560582653, 70: 0.9989884003237118, 71: 0.983585054631587, 72: 0.9671357724885526, 73: 0.9773590909090909, 74: 0.9583318166927529, 75: 0.9728395061728395, 76: 0.5820171108917407, 77: 0.9504825883409223, 78: 0.9932048430936496, 79: 0.9120496277915633, 80: 0.9909861128806217, 81: 0.8492459826946848, 82: 0.9506283748726916, 83: 0.968341150333251, 84: 0.9899628252788104, 85: 0.8921685254027262, 86: 0.999541511771995, 87: 0.5640075738865564, 88: 0.9239900867410162, 89: 0.9333497901752653, 90: 0.9511111111111111, 91: 0.9490234857849198, 92: 0.9169958035053074, 93: 0.9655486542443064, 94: 0.9604906211597765, 95: 0.953802328461729, 96: 0.9732509270704574, 97: 0.9120289734870477, 98: 0.9028566724294065, 99: 0.9700990099009901, 100: 0.8498518884226117, 101: 0.9632322155825374, 102: 0.9491598715097603, 103: 0.9887370600414078, 104: 0.9533828382838284, 105: 0.9253270401771407, 106: 0.9954706352529212, 107: 0.9929771315640881, 108: 0.7305789213260763, 109: 0.7434712027103331, 110: 0.9937773918795998, 111: 0.9195379537953795, 112: 0.9169679899516614, 113: 0.8990932161426096, 114: 0.991798712552612, 115: 0.9463707989190423, 116: 0.7869844406026179, 117: 0.4838368153585545, 118: 0.9184611018482387, 119: 0.9162512815074062, 120: 0.8401352131255668, 121: 0.8703383551494196, 122: 0.9746699751861042, 123: 0.9776708610905502, 124: 0.9825679012345679, 125: 0.9257429818062073, 126: 0.9883955808393108, 127: 0.9833004277722934, 128: 0.8722483304476873, 129: 0.9933382679496668, 130: 0.8926375899542581, 131: 0.9497902788058228, 132: 0.7035070387750062, 133: 0.8335391454680168, 134: 0.5895411938825851, 135: 0.7112455750391042, 136: 0.7929961249896942, 137: 0.9754381634164403, 138: 0.9873488027647495, 139: 0.9382891804902204, 140: 0.9661914172783739, 141: 0.8931919092254563, 142: nan, 143: 0.9159463241952747, 144: 0.9922705911451892, 145: 0.8008390918065152, 146: 0.5202319269676783, 147: 0.7007155193683691, 148: nan, 149: 0.9999450760696436, 150: 0.998702742772424, 151: nan, 152: 0.7911319512997697, 153: 0.3177109028120375, 154: 0.4390424481737414, 155: 0.9015739695087521, 156: 0.7857142857142857, 157: 0.8939753086419753, 158: 0.9282962962962963, 159: nan, 160: 0.6093017517887984, 161: 0.6414814814814814, 162: 0.9493254359986838, 163: 0.8729382716049382, 164: 0.8282266966086858, 165: 0.9864405732641462, 166: 0.4917772444946358, 167: 0.8979099678456591, 168: 0.9290749443482562, 169: 0.9186630443730962, 170: 0.4034189089113799, 171: 0.9742166296570443, 172: 0.9576340901612372, 173: 0.5530471255859857, 174: 0.8260547742413027, 175: 0.9857260726072608, 176: 0.9477904196553714, 177: 0.9996295381575697, 178: 0.8850234394275845, 179: 0.9884246188594016, 180: 0.924891525237546, 181: nan, 182: 0.6805692662059888, 183: 0.9971186301144315, 184: 0.9928465712876172, 185: 0.7084670451740311, 186: 0.8897385298470646, 187: 0.6831646507035299, 188: 0.7252652356279299, 189: 0.941037037037037, 190: 0.9990937551491185, 191: 0.8644499381953028, 192: 0.8492338111715274, 193: 0.854341736694678, 194: 0.9088329632371083, 195: 0.9786577843572662, 196: 0.785114786472476, 197: 0.6271897359980262, 198: 0.8628467934469416, 199: nan, 200: 0.9844559585492227, 201: 0.9845343863112866, 202: 0.4850727855909203, 203: nan, 204: 0.6252470355731226, 205: 0.9749444581584794, 206: 0.2883349786113853, 207: 0.9153622233207008, 208: nan, 209: 0.2427232363098175, 210: 0.9380858411445486, 211: nan, 212: 0.5289371861364947, 213: 0.06536753823384311, 214: 0.9028120374938331, 215: 0.9596595114729829, 216: 0.9903774981495189, 217: 0.297483966452886, 218: 0.3169708929452393, 219: 0.8766650222002961, 220: nan, 221: 0.723729649728663, 222: 0.6497286630488406, 223: 0.9933250927070458, 224: 0.9966057763515181, 225: 0.0461386627189736, 226: 0.8897113249444856, 227: 0.8415597235932872, 228: 0.6853333333333333, 229: 0.9953932214544258, 230: 0.9457326097681302, 231: nan, 232: 0.30340404538727184, 233: 0.9753208292201383, 234: 0.576486553170491, 235: nan, 236: 0.24685418208734272, 237: 0.6566354218056241, 238: 0.7185262898049865, 239: 0.926474216629657, 240: 0.9980266403552047, 241: 0.8376911692155895, 242: nan, 243: 0.8670448939319191, 244: 0.9639861864824864, 245: 0.9972345679012345, 246: 0.723729649728663, 247: 0.9447322970639033, 248: 0.7459906242289662, 249: 0.8572135609997525, 'micro': 0.9381054748198948}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e239c6c936f241f092a4cfbc9135f1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Iteration', max=507, style=ProgressStyle(description_width='i…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:22:19 - INFO - __main__ -   Loss after epoc 0.014155010520104002\n",
      "02/11/2019 00:22:19 - INFO - __main__ -   Eval after epoc 4\n",
      "02/11/2019 00:22:21 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2019 00:22:21 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:22:21 - INFO - __main__ -     Batch size = 32\n",
      "02/11/2019 00:22:49 - INFO - __main__ -   ***** Eval results *****\n",
      "02/11/2019 00:22:49 - INFO - __main__ -     eval_accuracy = 0.9968473601429442\n",
      "02/11/2019 00:22:49 - INFO - __main__ -     eval_loss = 0.013736800957969792\n",
      "02/11/2019 00:22:49 - INFO - __main__ -     roc_auc = {0: 0.9923784409570363, 1: 0.978441947565543, 2: 0.985863639212082, 3: 0.9782843269490153, 4: 0.9761588089330024, 5: 0.9353863243643545, 6: 0.9958963863392045, 7: 0.9431134782437798, 8: 0.9899188750390024, 9: 0.9369592088998764, 10: 0.9778393524283935, 11: 0.968180261104556, 12: 0.9357518920697597, 13: nan, 14: 0.9957037037037038, 15: 0.9988730277986477, 16: nan, 17: 0.9948618693134822, 18: 0.8848051307350765, 19: 0.9948263382103402, 20: 0.9509676073235617, 21: 0.7506294397985792, 22: 0.9923028785982478, 23: 0.9257060041407867, 24: 0.995553179281206, 25: 0.9796273291925466, 26: 0.9974974778245089, 27: 0.9911711499134812, 28: 0.9987692307692309, 29: 0.9868262624831787, 30: 0.8312845065562458, 31: 0.9907740430171069, 32: 0.9644707347447072, 33: 0.741607010614663, 34: 0.9909321614260955, 35: 0.9884444444444445, 36: 0.9813321756387993, 37: 0.9920308244615004, 38: 0.993684417199328, 39: 0.9370083177422628, 40: 0.9799432238953345, 41: 0.9961281488301815, 42: 0.9986743848613991, 43: 0.9948501131805453, 44: 0.9854431084391004, 45: 0.9979774263811491, 46: 0.9952006389102361, 47: 0.9490502752561157, 48: 0.9279868022895476, 49: 0.965629373507862, 50: nan, 51: 0.9897454904867804, 52: 0.9634387351778656, 53: 0.9401086688071129, 54: 0.9991158503911692, 55: 0.7811034312515428, 56: 0.9231366459627329, 57: 0.9945817692471138, 58: 0.9991730753328372, 59: 0.989494062374578, 60: 0.8989817792068596, 61: 0.8302145214521451, 62: 0.9887267747951932, 63: 0.9798453438631128, 64: 0.8082572204393977, 65: 0.904679559570864, 66: 0.922008018893832, 67: 0.994589992859889, 68: 0.9878992324832879, 69: 0.9883954976826308, 70: 0.9970551209423613, 71: 0.9882236456714973, 72: 0.9727752827069649, 73: 0.9904318181818182, 74: 0.9690696320023294, 75: 0.989283950617284, 76: 0.6074366567949984, 77: 0.9795581350606041, 78: 0.9962626637015073, 79: 0.9373101736972704, 80: 0.9965866140060305, 81: 0.9043016069221261, 82: 0.9680480024596938, 83: 0.9574796346581091, 84: 0.9975216852540272, 85: 0.927087980173482, 86: 0.9994919454770755, 87: 0.598419362805631, 88: 0.9100123915737299, 89: 0.926746482350037, 90: 0.9470123456790123, 91: 0.9862051915945611, 92: 0.9126758824981486, 93: 0.982592132505176, 94: 0.9393911289387651, 95: 0.9507747777502547, 96: 0.9902348578491966, 97: 0.9562494816289293, 98: 0.9561620153124228, 99: 0.9194059405940594, 100: 0.7885090101209578, 101: 0.9681041373272942, 102: 0.9418396342970102, 103: 0.9866501035196686, 104: 0.9556930693069308, 105: 0.9688255688605308, 106: 0.9963079968028014, 107: 0.998235460191982, 108: 0.9319072812392951, 109: 0.8192758328627895, 110: 0.9983048044323163, 111: 0.9763036303630362, 112: 0.901457770334564, 113: 0.8835262441198317, 114: 0.996456424857638, 115: 0.9878392265824231, 116: 0.7578002799045032, 117: 0.5413961038961039, 118: 0.9661037993543581, 119: 0.9362958249372504, 120: 0.8910668645395332, 121: 0.8968469580966495, 122: 0.9996724565756823, 123: 0.9830989390574882, 124: 0.9998518518518519, 125: 0.9306413106116735, 126: 0.9903536977491961, 127: 0.9708785784797631, 128: 0.9100296809300026, 129: 0.9988897113249445, 130: 0.9080499956297526, 131: 0.9887737478411055, 132: 0.9053264180456081, 133: 0.9125710051864658, 134: 0.757523433645782, 135: 0.7061414340989545, 136: 0.8581498886965124, 137: 0.9769809923475685, 138: 0.9896321895828191, 139: 0.9615777420153504, 140: 0.965238565782044, 141: 0.8919585594474593, 142: nan, 143: 0.8541615213632996, 144: 0.9943729903536977, 145: 0.7788746298124383, 146: 0.683691093017518, 147: 0.6934369602763386, 148: nan, 149: 1.0, 150: 0.9972510501606129, 151: nan, 152: 0.9428265876933202, 153: 0.24198322644301928, 154: 0.5817703191839421, 155: 0.9428994918125353, 156: 0.7458672588206268, 157: 0.892246913580247, 158: 0.9134320987654321, 159: nan, 160: 0.70046878855169, 161: 0.5489876543209877, 162: 0.9893879565646594, 163: 0.9672098765432099, 164: 0.8703802382674228, 165: 0.979676303434643, 166: 0.5587238848108413, 167: 0.9362066122516283, 168: 0.9674540357820101, 169: 0.92994154935375, 170: 0.4738336213280671, 171: 0.9875400937577103, 172: 0.9113195129976965, 173: 0.5743893412287195, 174: 0.688378978534419, 175: 0.9928382838283828, 176: 0.9374639294253442, 177: 1.0, 178: 0.883172958302492, 179: 0.9867659514398645, 180: 0.9240951282473774, 181: nan, 182: 0.6272622573214873, 183: 0.9970774676874948, 184: 0.9755796743956586, 185: 0.7762898049864231, 186: 0.9972866304884065, 187: 0.6387311774870402, 188: 0.6384159881569208, 189: 0.9772345679012345, 190: 0.9995606085571483, 191: 0.8910012360939432, 192: 0.8934475751084747, 193: 0.920195529192069, 194: 0.9422649888971133, 195: 0.9823587466074513, 196: 0.7854850654159466, 197: 0.650505798174192, 198: 0.9142586646908701, 199: nan, 200: 0.9959289415247964, 201: 0.9931720960842382, 202: 0.6218850234394275, 203: nan, 204: 0.7094155844155844, 205: 0.9769192791903234, 206: 0.29508061862454754, 207: 0.9977206568902071, 208: nan, 209: 0.1778490379871731, 210: 0.9639861864824865, 211: nan, 212: 0.5996542356137318, 213: 0.2022693635915146, 214: 0.8704982733103108, 215: 0.9818652849740933, 216: 0.998519615099926, 217: 0.6356684755796744, 218: 0.5542673902318698, 219: 0.9933399111988158, 220: nan, 221: 0.7927972372964973, 222: 0.5281203749383325, 223: 0.9940173053152039, 224: 0.9936435448037522, 225: 0.06908462867012088, 226: 0.8298791018998273, 227: 0.9215202369200395, 228: 0.774320987654321, 229: 0.997861138532412, 230: 0.9121854958066108, 231: nan, 232: 0.4368524913665516, 233: 0.9581276735768346, 234: 0.6695040710584752, 235: nan, 236: 0.22107081174438686, 237: 0.5434139121854957, 238: 0.7483337447543816, 239: 0.9928448063163089, 240: 0.994573260976813, 241: 0.870004933399112, 242: nan, 243: 0.9348791317217563, 244: 0.9948199309324124, 245: 0.9984197530864198, 246: 0.88751850024667, 247: 0.9733530717986677, 248: 0.9343696027633851, 249: 0.8701346908473857, 'micro': 0.9495836062957786}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:22:50 - INFO - pytorch_pretrained_bert.modeling -   loading archive file ../models/pretrained-weights/uncased_L-12_H-768_A-12 from cache at ../models/pretrained-weights/uncased_L-12_H-768_A-12\n",
      "02/11/2019 00:22:50 - INFO - pytorch_pretrained_bert.modeling -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=250, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save a trained model\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
    "output_model_file = os.path.join(PYTORCH_PRETRAINED_BERT_CACHE, \"finetuned_pytorch_model.bin\")\n",
    "torch.save(model_to_save.state_dict(), output_model_file)\n",
    "\n",
    "# Load a trained model that you have fine-tuned\n",
    "model_state_dict = torch.load(output_model_file)\n",
    "model = BertForMultiLabelSequenceClassification.from_pretrained(args['bert_model'], num_labels = num_labels, state_dict=model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMultiLabelSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1)\n",
       "  (classifier): Linear(in_features=768, out_features=250, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:22:54 - INFO - __main__ -   ***** Running evaluation *****\n",
      "02/11/2019 00:22:54 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:22:54 - INFO - __main__ -     Batch size = 32\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sklearn/metrics/ranking.py:547: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n",
      "02/11/2019 00:24:21 - INFO - __main__ -   ***** Eval results *****\n",
      "02/11/2019 00:24:21 - INFO - __main__ -     eval_accuracy = 0.9968473601429442\n",
      "02/11/2019 00:24:21 - INFO - __main__ -     eval_loss = 0.0137365571861192\n",
      "02/11/2019 00:24:21 - INFO - __main__ -     roc_auc = {0: 0.9923784409570362, 1: 0.978441947565543, 2: 0.985863639212082, 3: 0.9782843269490153, 4: 0.9761588089330024, 5: 0.9353863243643545, 6: 0.9958963863392045, 7: 0.9431134782437797, 8: 0.9899188750390024, 9: 0.9369592088998764, 10: 0.9778393524283935, 11: 0.968180261104556, 12: 0.9357518920697597, 13: nan, 14: 0.9957037037037038, 15: 0.9988730277986476, 16: nan, 17: 0.9948618693134821, 18: 0.8848051307350765, 19: 0.9948263382103402, 20: 0.9509676073235617, 21: 0.7506294397985793, 22: 0.9923028785982478, 23: 0.9257060041407867, 24: 0.995553179281206, 25: 0.9796273291925466, 26: 0.997497477824509, 27: 0.9911711499134811, 28: 0.9987692307692307, 29: 0.9868262624831787, 30: 0.8312845065562456, 31: 0.9907740430171069, 32: 0.9644707347447072, 33: 0.741607010614663, 34: 0.9909321614260955, 35: 0.9884197530864197, 36: 0.9813321756387993, 37: 0.9920308244615003, 38: 0.9936844171993281, 39: 0.9370083177422627, 40: 0.9799432238953344, 41: 0.9961281488301815, 42: 0.9986743848613991, 43: 0.9948501131805453, 44: 0.9854431084391004, 45: 0.9979774263811491, 46: 0.9952006389102362, 47: 0.9490502752561158, 48: 0.9279837641425742, 49: 0.965629373507862, 50: nan, 51: 0.9897454904867803, 52: 0.9634387351778656, 53: 0.9401086688071129, 54: 0.9991158503911692, 55: 0.7811034312515428, 56: 0.9231366459627328, 57: 0.994581769247114, 58: 0.9991730753328373, 59: 0.9894940623745779, 60: 0.8989817792068596, 61: 0.8302145214521454, 62: 0.9887267747951932, 63: 0.979845343863113, 64: 0.8082572204393977, 65: 0.904679559570864, 66: 0.9220080188938321, 67: 0.994589992859889, 68: 0.9878992324832879, 69: 0.9883954976826308, 70: 0.9970551209423613, 71: 0.9882236456714973, 72: 0.9727752827069649, 73: 0.9904318181818182, 74: 0.9690696320023294, 75: 0.989283950617284, 76: 0.6074366567949984, 77: 0.9795581350606041, 78: 0.9962626637015073, 79: 0.9373101736972704, 80: 0.9965866140060305, 81: 0.9043016069221261, 82: 0.9680480024596938, 83: 0.957479634658109, 84: 0.9975216852540273, 85: 0.927087980173482, 86: 0.9994919454770756, 87: 0.598419362805631, 88: 0.9100123915737299, 89: 0.9267464823500371, 90: 0.9470123456790124, 91: 0.9862051915945611, 92: 0.9126758824981486, 93: 0.982592132505176, 94: 0.9393911289387651, 95: 0.9507747777502547, 96: 0.9902348578491966, 97: 0.9562494816289293, 98: 0.9561620153124227, 99: 0.9194059405940594, 100: 0.7885090101209578, 101: 0.9681041373272942, 102: 0.9418396342970101, 103: 0.9866501035196688, 104: 0.9556930693069307, 105: 0.9688255688605308, 106: 0.9963079968028014, 107: 0.998235460191982, 108: 0.9319072812392951, 109: 0.8192758328627894, 110: 0.9983048044323162, 111: 0.9763036303630362, 112: 0.901457770334564, 113: 0.8835262441198317, 114: 0.996456424857638, 115: 0.987839226582423, 116: 0.7578002799045032, 117: 0.5413961038961039, 118: 0.9661037993543582, 119: 0.9362958249372504, 120: 0.8910668645395332, 121: 0.8968469580966494, 122: 0.9996724565756824, 123: 0.9830989390574882, 124: 0.9998518518518518, 125: 0.9306413106116737, 126: 0.9903536977491962, 127: 0.970878578479763, 128: 0.9100296809300025, 129: 0.9988897113249444, 130: 0.9080499956297527, 131: 0.9887737478411054, 132: 0.9053264180456081, 133: 0.9125710051864658, 134: 0.7575234336457819, 135: 0.7061414340989545, 136: 0.8581498886965124, 137: 0.9769809923475685, 138: 0.9896321895828191, 139: 0.9615777420153504, 140: 0.965238565782044, 141: 0.8919585594474593, 142: nan, 143: 0.8541615213632996, 144: 0.9943729903536977, 145: 0.7788746298124383, 146: 0.6836910930175178, 147: 0.6934369602763385, 148: nan, 149: 0.9999999999999999, 150: 0.9972510501606129, 151: nan, 152: 0.9428265876933202, 153: 0.24198322644301928, 154: 0.5817703191839421, 155: 0.9428994918125353, 156: 0.7458672588206268, 157: 0.892246913580247, 158: 0.9134320987654321, 159: nan, 160: 0.7004687885516901, 161: 0.5489876543209877, 162: 0.9893879565646594, 163: 0.9672098765432099, 164: 0.8703802382674228, 165: 0.979676303434643, 166: 0.5587238848108413, 167: 0.9362066122516284, 168: 0.9674540357820101, 169: 0.9299415493537498, 170: 0.4738336213280671, 171: 0.9875400937577103, 172: 0.9113195129976966, 173: 0.5743893412287195, 174: 0.688378978534419, 175: 0.9928382838283829, 176: 0.9374639294253442, 177: 1.0, 178: 0.883172958302492, 179: 0.9867659514398645, 180: 0.9240951282473774, 181: nan, 182: 0.6272622573214873, 183: 0.9970774676874947, 184: 0.9755796743956585, 185: 0.7762898049864231, 186: 0.9972866304884065, 187: 0.6387311774870402, 188: 0.6384159881569208, 189: 0.9772345679012345, 190: 0.9995606085571483, 191: 0.8910012360939431, 192: 0.8934475751084747, 193: 0.9201955291920689, 194: 0.9422649888971132, 195: 0.9823587466074513, 196: 0.7854850654159466, 197: 0.650505798174192, 198: 0.9142586646908701, 199: nan, 200: 0.9959289415247965, 201: 0.9931720960842383, 202: 0.6218850234394275, 203: nan, 204: 0.7094155844155844, 205: 0.9769192791903233, 206: 0.2950806186245476, 207: 0.9977206568902072, 208: nan, 209: 0.1778490379871731, 210: 0.9639861864824865, 211: nan, 212: 0.5996542356137318, 213: 0.2022693635915146, 214: 0.8704982733103108, 215: 0.9818652849740933, 216: 0.998519615099926, 217: 0.6356684755796744, 218: 0.5542673902318698, 219: 0.993339911198816, 220: nan, 221: 0.7927972372964973, 222: 0.5281203749383325, 223: 0.9940173053152039, 224: 0.9936435448037522, 225: 0.06908462867012088, 226: 0.8298791018998273, 227: 0.9215202369200395, 228: 0.774320987654321, 229: 0.997861138532412, 230: 0.9121854958066107, 231: nan, 232: 0.4368524913665516, 233: 0.9581276735768345, 234: 0.6695040710584752, 235: nan, 236: 0.22107081174438686, 237: 0.5434139121854957, 238: 0.7483337447543816, 239: 0.992844806316309, 240: 0.994573260976813, 241: 0.870004933399112, 242: nan, 243: 0.9348791317217563, 244: 0.9948199309324125, 245: 0.9984197530864198, 246: 0.88751850024667, 247: 0.9733530717986677, 248: 0.9343696027633851, 249: 0.8701346908473857, 'micro': 0.9495836109363616}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0137365571861192,\n",
       " 'eval_accuracy': 0.9968473601429442,\n",
       " 'roc_auc': {0: 0.9923784409570362,\n",
       "  1: 0.978441947565543,\n",
       "  2: 0.985863639212082,\n",
       "  3: 0.9782843269490153,\n",
       "  4: 0.9761588089330024,\n",
       "  5: 0.9353863243643545,\n",
       "  6: 0.9958963863392045,\n",
       "  7: 0.9431134782437797,\n",
       "  8: 0.9899188750390024,\n",
       "  9: 0.9369592088998764,\n",
       "  10: 0.9778393524283935,\n",
       "  11: 0.968180261104556,\n",
       "  12: 0.9357518920697597,\n",
       "  13: nan,\n",
       "  14: 0.9957037037037038,\n",
       "  15: 0.9988730277986476,\n",
       "  16: nan,\n",
       "  17: 0.9948618693134821,\n",
       "  18: 0.8848051307350765,\n",
       "  19: 0.9948263382103402,\n",
       "  20: 0.9509676073235617,\n",
       "  21: 0.7506294397985793,\n",
       "  22: 0.9923028785982478,\n",
       "  23: 0.9257060041407867,\n",
       "  24: 0.995553179281206,\n",
       "  25: 0.9796273291925466,\n",
       "  26: 0.997497477824509,\n",
       "  27: 0.9911711499134811,\n",
       "  28: 0.9987692307692307,\n",
       "  29: 0.9868262624831787,\n",
       "  30: 0.8312845065562456,\n",
       "  31: 0.9907740430171069,\n",
       "  32: 0.9644707347447072,\n",
       "  33: 0.741607010614663,\n",
       "  34: 0.9909321614260955,\n",
       "  35: 0.9884197530864197,\n",
       "  36: 0.9813321756387993,\n",
       "  37: 0.9920308244615003,\n",
       "  38: 0.9936844171993281,\n",
       "  39: 0.9370083177422627,\n",
       "  40: 0.9799432238953344,\n",
       "  41: 0.9961281488301815,\n",
       "  42: 0.9986743848613991,\n",
       "  43: 0.9948501131805453,\n",
       "  44: 0.9854431084391004,\n",
       "  45: 0.9979774263811491,\n",
       "  46: 0.9952006389102362,\n",
       "  47: 0.9490502752561158,\n",
       "  48: 0.9279837641425742,\n",
       "  49: 0.965629373507862,\n",
       "  50: nan,\n",
       "  51: 0.9897454904867803,\n",
       "  52: 0.9634387351778656,\n",
       "  53: 0.9401086688071129,\n",
       "  54: 0.9991158503911692,\n",
       "  55: 0.7811034312515428,\n",
       "  56: 0.9231366459627328,\n",
       "  57: 0.994581769247114,\n",
       "  58: 0.9991730753328373,\n",
       "  59: 0.9894940623745779,\n",
       "  60: 0.8989817792068596,\n",
       "  61: 0.8302145214521454,\n",
       "  62: 0.9887267747951932,\n",
       "  63: 0.979845343863113,\n",
       "  64: 0.8082572204393977,\n",
       "  65: 0.904679559570864,\n",
       "  66: 0.9220080188938321,\n",
       "  67: 0.994589992859889,\n",
       "  68: 0.9878992324832879,\n",
       "  69: 0.9883954976826308,\n",
       "  70: 0.9970551209423613,\n",
       "  71: 0.9882236456714973,\n",
       "  72: 0.9727752827069649,\n",
       "  73: 0.9904318181818182,\n",
       "  74: 0.9690696320023294,\n",
       "  75: 0.989283950617284,\n",
       "  76: 0.6074366567949984,\n",
       "  77: 0.9795581350606041,\n",
       "  78: 0.9962626637015073,\n",
       "  79: 0.9373101736972704,\n",
       "  80: 0.9965866140060305,\n",
       "  81: 0.9043016069221261,\n",
       "  82: 0.9680480024596938,\n",
       "  83: 0.957479634658109,\n",
       "  84: 0.9975216852540273,\n",
       "  85: 0.927087980173482,\n",
       "  86: 0.9994919454770756,\n",
       "  87: 0.598419362805631,\n",
       "  88: 0.9100123915737299,\n",
       "  89: 0.9267464823500371,\n",
       "  90: 0.9470123456790124,\n",
       "  91: 0.9862051915945611,\n",
       "  92: 0.9126758824981486,\n",
       "  93: 0.982592132505176,\n",
       "  94: 0.9393911289387651,\n",
       "  95: 0.9507747777502547,\n",
       "  96: 0.9902348578491966,\n",
       "  97: 0.9562494816289293,\n",
       "  98: 0.9561620153124227,\n",
       "  99: 0.9194059405940594,\n",
       "  100: 0.7885090101209578,\n",
       "  101: 0.9681041373272942,\n",
       "  102: 0.9418396342970101,\n",
       "  103: 0.9866501035196688,\n",
       "  104: 0.9556930693069307,\n",
       "  105: 0.9688255688605308,\n",
       "  106: 0.9963079968028014,\n",
       "  107: 0.998235460191982,\n",
       "  108: 0.9319072812392951,\n",
       "  109: 0.8192758328627894,\n",
       "  110: 0.9983048044323162,\n",
       "  111: 0.9763036303630362,\n",
       "  112: 0.901457770334564,\n",
       "  113: 0.8835262441198317,\n",
       "  114: 0.996456424857638,\n",
       "  115: 0.987839226582423,\n",
       "  116: 0.7578002799045032,\n",
       "  117: 0.5413961038961039,\n",
       "  118: 0.9661037993543582,\n",
       "  119: 0.9362958249372504,\n",
       "  120: 0.8910668645395332,\n",
       "  121: 0.8968469580966494,\n",
       "  122: 0.9996724565756824,\n",
       "  123: 0.9830989390574882,\n",
       "  124: 0.9998518518518518,\n",
       "  125: 0.9306413106116737,\n",
       "  126: 0.9903536977491962,\n",
       "  127: 0.970878578479763,\n",
       "  128: 0.9100296809300025,\n",
       "  129: 0.9988897113249444,\n",
       "  130: 0.9080499956297527,\n",
       "  131: 0.9887737478411054,\n",
       "  132: 0.9053264180456081,\n",
       "  133: 0.9125710051864658,\n",
       "  134: 0.7575234336457819,\n",
       "  135: 0.7061414340989545,\n",
       "  136: 0.8581498886965124,\n",
       "  137: 0.9769809923475685,\n",
       "  138: 0.9896321895828191,\n",
       "  139: 0.9615777420153504,\n",
       "  140: 0.965238565782044,\n",
       "  141: 0.8919585594474593,\n",
       "  142: nan,\n",
       "  143: 0.8541615213632996,\n",
       "  144: 0.9943729903536977,\n",
       "  145: 0.7788746298124383,\n",
       "  146: 0.6836910930175178,\n",
       "  147: 0.6934369602763385,\n",
       "  148: nan,\n",
       "  149: 0.9999999999999999,\n",
       "  150: 0.9972510501606129,\n",
       "  151: nan,\n",
       "  152: 0.9428265876933202,\n",
       "  153: 0.24198322644301928,\n",
       "  154: 0.5817703191839421,\n",
       "  155: 0.9428994918125353,\n",
       "  156: 0.7458672588206268,\n",
       "  157: 0.892246913580247,\n",
       "  158: 0.9134320987654321,\n",
       "  159: nan,\n",
       "  160: 0.7004687885516901,\n",
       "  161: 0.5489876543209877,\n",
       "  162: 0.9893879565646594,\n",
       "  163: 0.9672098765432099,\n",
       "  164: 0.8703802382674228,\n",
       "  165: 0.979676303434643,\n",
       "  166: 0.5587238848108413,\n",
       "  167: 0.9362066122516284,\n",
       "  168: 0.9674540357820101,\n",
       "  169: 0.9299415493537498,\n",
       "  170: 0.4738336213280671,\n",
       "  171: 0.9875400937577103,\n",
       "  172: 0.9113195129976966,\n",
       "  173: 0.5743893412287195,\n",
       "  174: 0.688378978534419,\n",
       "  175: 0.9928382838283829,\n",
       "  176: 0.9374639294253442,\n",
       "  177: 1.0,\n",
       "  178: 0.883172958302492,\n",
       "  179: 0.9867659514398645,\n",
       "  180: 0.9240951282473774,\n",
       "  181: nan,\n",
       "  182: 0.6272622573214873,\n",
       "  183: 0.9970774676874947,\n",
       "  184: 0.9755796743956585,\n",
       "  185: 0.7762898049864231,\n",
       "  186: 0.9972866304884065,\n",
       "  187: 0.6387311774870402,\n",
       "  188: 0.6384159881569208,\n",
       "  189: 0.9772345679012345,\n",
       "  190: 0.9995606085571483,\n",
       "  191: 0.8910012360939431,\n",
       "  192: 0.8934475751084747,\n",
       "  193: 0.9201955291920689,\n",
       "  194: 0.9422649888971132,\n",
       "  195: 0.9823587466074513,\n",
       "  196: 0.7854850654159466,\n",
       "  197: 0.650505798174192,\n",
       "  198: 0.9142586646908701,\n",
       "  199: nan,\n",
       "  200: 0.9959289415247965,\n",
       "  201: 0.9931720960842383,\n",
       "  202: 0.6218850234394275,\n",
       "  203: nan,\n",
       "  204: 0.7094155844155844,\n",
       "  205: 0.9769192791903233,\n",
       "  206: 0.2950806186245476,\n",
       "  207: 0.9977206568902072,\n",
       "  208: nan,\n",
       "  209: 0.1778490379871731,\n",
       "  210: 0.9639861864824865,\n",
       "  211: nan,\n",
       "  212: 0.5996542356137318,\n",
       "  213: 0.2022693635915146,\n",
       "  214: 0.8704982733103108,\n",
       "  215: 0.9818652849740933,\n",
       "  216: 0.998519615099926,\n",
       "  217: 0.6356684755796744,\n",
       "  218: 0.5542673902318698,\n",
       "  219: 0.993339911198816,\n",
       "  220: nan,\n",
       "  221: 0.7927972372964973,\n",
       "  222: 0.5281203749383325,\n",
       "  223: 0.9940173053152039,\n",
       "  224: 0.9936435448037522,\n",
       "  225: 0.06908462867012088,\n",
       "  226: 0.8298791018998273,\n",
       "  227: 0.9215202369200395,\n",
       "  228: 0.774320987654321,\n",
       "  229: 0.997861138532412,\n",
       "  230: 0.9121854958066107,\n",
       "  231: nan,\n",
       "  232: 0.4368524913665516,\n",
       "  233: 0.9581276735768345,\n",
       "  234: 0.6695040710584752,\n",
       "  235: nan,\n",
       "  236: 0.22107081174438686,\n",
       "  237: 0.5434139121854957,\n",
       "  238: 0.7483337447543816,\n",
       "  239: 0.992844806316309,\n",
       "  240: 0.994573260976813,\n",
       "  241: 0.870004933399112,\n",
       "  242: nan,\n",
       "  243: 0.9348791317217563,\n",
       "  244: 0.9948199309324125,\n",
       "  245: 0.9984197530864198,\n",
       "  246: 0.88751850024667,\n",
       "  247: 0.9733530717986677,\n",
       "  248: 0.9343696027633851,\n",
       "  249: 0.8701346908473857,\n",
       "  'micro': 0.9495836109363616}}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, path, test_filename='test.csv'):\n",
    "    predict_processor = MultiLabelTextProcessor(path)\n",
    "    test_examples = predict_processor.get_test_examples(path, test_filename, size=-1)\n",
    "    \n",
    "    # Hold input data for returning it \n",
    "    input_data = [{ 'id': input_example.guid, 'comment_text': input_example.text_a } for input_example in test_examples]\n",
    "\n",
    "    test_features = convert_examples_to_features(\n",
    "        test_examples, label_list, args['max_seq_length'], tokenizer)\n",
    "    \n",
    "    logger.info(\"***** Running prediction *****\")\n",
    "    logger.info(\"  Num examples = %d\", len(test_examples))\n",
    "    logger.info(\"  Batch size = %d\", args['eval_batch_size'])\n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=args['eval_batch_size'])\n",
    "    \n",
    "    all_logits = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(tqdm(test_dataloader, desc=\"Prediction Iteration\")):\n",
    "        input_ids, input_mask, segment_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            logits = logits.sigmoid()\n",
    "\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    return pd.merge(pd.DataFrame(input_data), pd.DataFrame(all_logits, columns=label_list), left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "02/11/2019 00:24:22 - INFO - __main__ -   ***** Running prediction *****\n",
      "02/11/2019 00:24:22 - INFO - __main__ -     Num examples = 4055\n",
      "02/11/2019 00:24:22 - INFO - __main__ -     Batch size = 32\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a05c945291c34b0eaf7a666e5465adec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Prediction Iteration', max=127, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "result = predict(model, args['data_dir'], 'val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_batch(model, label_list, texts, max_seq_length=512, batch_size=32, device='cuda'):\n",
    "    \n",
    "    examples = []\n",
    "    input_data = []\n",
    "    \n",
    "    for index, text in enumerate(texts):\n",
    "        examples.append(InputExample(index, text, labels=[]))\n",
    "        input_data.append({\n",
    "            'id': index, \n",
    "            'text': text\n",
    "        })\n",
    "           \n",
    "    test_features = convert_examples_to_features(examples, label_list, max_seq_length, tokenizer)\n",
    "    \n",
    "    \n",
    "    all_input_ids = torch.tensor([f.input_ids for f in test_features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in test_features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in test_features], dtype=torch.long)\n",
    "\n",
    "    test_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids)\n",
    "    \n",
    "    # Run prediction for full data\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    all_logits = None\n",
    "    \n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for step, batch in enumerate(test_dataloader):\n",
    "        input_ids, input_mask, segment_ids = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        input_mask = input_mask.to(device)\n",
    "        segment_ids = segment_ids.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(input_ids, segment_ids, input_mask)\n",
    "            logits = logits.sigmoid()\n",
    "\n",
    "        if all_logits is None:\n",
    "            all_logits = logits.detach().cpu().numpy()\n",
    "        else:\n",
    "            all_logits = np.concatenate((all_logits, logits.detach().cpu().numpy()), axis=0)\n",
    "            \n",
    "        nb_eval_examples += input_ids.size(0)\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    result_df =  pd.DataFrame(all_logits, columns=label_list)\n",
    "    results = result_df.to_dict('record')\n",
    "    \n",
    "    return [sorted(x.items(), key=lambda kv: kv[1], reverse=True) for x in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>id</th>\n",
       "      <th>enquire-bill</th>\n",
       "      <th>problem-hot_water</th>\n",
       "      <th>enquire-agreement</th>\n",
       "      <th>enquire-safety_certificate</th>\n",
       "      <th>enquire-appointment_when</th>\n",
       "      <th>enquire-heating</th>\n",
       "      <th>request-card</th>\n",
       "      <th>enquire-vague</th>\n",
       "      <th>...</th>\n",
       "      <th>enquire-hive_sensor</th>\n",
       "      <th>enquire-hive_bulb</th>\n",
       "      <th>enquire-hive_thermostat</th>\n",
       "      <th>enquire-hive_app</th>\n",
       "      <th>enquire-install_hive</th>\n",
       "      <th>problem-hive_thermostat</th>\n",
       "      <th>cancel-hive</th>\n",
       "      <th>problem-hive_sensor</th>\n",
       "      <th>problem-hive_camera</th>\n",
       "      <th>enquire-business_care</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>refund of excess</td>\n",
       "      <td>test-0</td>\n",
       "      <td>0.016918</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.004922</td>\n",
       "      <td>0.002933</td>\n",
       "      <td>0.001359</td>\n",
       "      <td>0.001845</td>\n",
       "      <td>0.009211</td>\n",
       "      <td>0.007514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.003488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>upset and angry</td>\n",
       "      <td>test-1</td>\n",
       "      <td>0.010182</td>\n",
       "      <td>0.003531</td>\n",
       "      <td>0.003536</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.004838</td>\n",
       "      <td>0.002393</td>\n",
       "      <td>0.003865</td>\n",
       "      <td>0.370754</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000664</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>to set_up a direct_debit for my gas</td>\n",
       "      <td>test-2</td>\n",
       "      <td>0.003219</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.009117</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.002058</td>\n",
       "      <td>0.011558</td>\n",
       "      <td>0.010287</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000506</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.004111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i've moved house and i just missed call from y...</td>\n",
       "      <td>test-3</td>\n",
       "      <td>0.012253</td>\n",
       "      <td>0.003399</td>\n",
       "      <td>0.005182</td>\n",
       "      <td>0.001347</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.009998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000561</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000473</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>we have a wrong bill</td>\n",
       "      <td>test-4</td>\n",
       "      <td>0.321525</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.002469</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.002236</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.012880</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.001063</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.001798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>i have a plumbing leak</td>\n",
       "      <td>test-5</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.048212</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.008075</td>\n",
       "      <td>0.002964</td>\n",
       "      <td>0.006388</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.002879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Hi i just moved 2 days ago new home i dis swit...</td>\n",
       "      <td>test-6</td>\n",
       "      <td>0.018381</td>\n",
       "      <td>0.004637</td>\n",
       "      <td>0.005581</td>\n",
       "      <td>0.001341</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.002491</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.010923</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.001778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i've got problem with my central heating boiler</td>\n",
       "      <td>test-7</td>\n",
       "      <td>0.006559</td>\n",
       "      <td>0.007286</td>\n",
       "      <td>0.003290</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.004138</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.000479</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.001135</td>\n",
       "      <td>0.000202</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.001464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>i just received my bill and it's about a</td>\n",
       "      <td>test-8</td>\n",
       "      <td>0.544706</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.002552</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.001665</td>\n",
       "      <td>0.006385</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000323</td>\n",
       "      <td>0.000567</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.000369</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.000248</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.001654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Hi my free electric that I got on a Sunday has...</td>\n",
       "      <td>test-9</td>\n",
       "      <td>0.023657</td>\n",
       "      <td>0.002069</td>\n",
       "      <td>0.015792</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.008237</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000424</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000383</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>0.000277</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.002390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>to end an account</td>\n",
       "      <td>test-10</td>\n",
       "      <td>0.016380</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.018733</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.001888</td>\n",
       "      <td>0.002913</td>\n",
       "      <td>0.004289</td>\n",
       "      <td>0.025088</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002315</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.004840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>speak about my homecare</td>\n",
       "      <td>test-11</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.003496</td>\n",
       "      <td>0.679273</td>\n",
       "      <td>0.006488</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.008706</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.000297</td>\n",
       "      <td>0.000752</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.008596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>\"Hi lakshmi, i am moving house - the app would...</td>\n",
       "      <td>test-12</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.010009</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.003351</td>\n",
       "      <td>0.002173</td>\n",
       "      <td>0.016839</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.002533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>homecare cancel</td>\n",
       "      <td>test-13</td>\n",
       "      <td>0.004981</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.054062</td>\n",
       "      <td>0.007985</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.007878</td>\n",
       "      <td>0.007157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.007769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>regarding e-mail regarding meter reading</td>\n",
       "      <td>test-14</td>\n",
       "      <td>0.013042</td>\n",
       "      <td>0.012599</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.001504</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000519</td>\n",
       "      <td>0.000613</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000377</td>\n",
       "      <td>0.000274</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.001884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>\"Hi, I cannot log into my account its not reco...</td>\n",
       "      <td>test-15</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.015621</td>\n",
       "      <td>0.003940</td>\n",
       "      <td>0.001607</td>\n",
       "      <td>0.002716</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.024591</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.000631</td>\n",
       "      <td>0.000606</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.003586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>it's about my gas and electricity tariff that ...</td>\n",
       "      <td>test-16</td>\n",
       "      <td>0.028830</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.011123</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.006737</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000400</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000348</td>\n",
       "      <td>0.000276</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000430</td>\n",
       "      <td>0.002180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>hear a transaction</td>\n",
       "      <td>test-17</td>\n",
       "      <td>0.022015</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.003755</td>\n",
       "      <td>0.004643</td>\n",
       "      <td>0.002839</td>\n",
       "      <td>0.002563</td>\n",
       "      <td>0.006636</td>\n",
       "      <td>0.062644</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.003599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>to give you a up to date meter reading</td>\n",
       "      <td>test-18</td>\n",
       "      <td>0.015410</td>\n",
       "      <td>0.012399</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.014121</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.007310</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001553</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.000478</td>\n",
       "      <td>0.001977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>credit meter</td>\n",
       "      <td>test-19</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.003851</td>\n",
       "      <td>0.005842</td>\n",
       "      <td>0.002120</td>\n",
       "      <td>0.015475</td>\n",
       "      <td>0.014025</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.001238</td>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>to amend my homecare agreement</td>\n",
       "      <td>test-20</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.205809</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000530</td>\n",
       "      <td>0.000738</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.006822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>i want to cancel my contract</td>\n",
       "      <td>test-21</td>\n",
       "      <td>0.004657</td>\n",
       "      <td>0.001920</td>\n",
       "      <td>0.072696</td>\n",
       "      <td>0.008365</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001822</td>\n",
       "      <td>0.007292</td>\n",
       "      <td>0.006895</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.008324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Hi sai Please can u send us the latest bill by...</td>\n",
       "      <td>test-22</td>\n",
       "      <td>0.041587</td>\n",
       "      <td>0.001573</td>\n",
       "      <td>0.002699</td>\n",
       "      <td>0.004127</td>\n",
       "      <td>0.002769</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.065462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001756</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.001866</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.001536</td>\n",
       "      <td>0.002783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>incorrect name and address for an electricity ...</td>\n",
       "      <td>test-23</td>\n",
       "      <td>0.053010</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.007808</td>\n",
       "      <td>0.003507</td>\n",
       "      <td>0.001990</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.003732</td>\n",
       "      <td>0.030908</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001824</td>\n",
       "      <td>0.000995</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.003039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>gas problem</td>\n",
       "      <td>test-24</td>\n",
       "      <td>0.003420</td>\n",
       "      <td>0.026205</td>\n",
       "      <td>0.002521</td>\n",
       "      <td>0.005462</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.003637</td>\n",
       "      <td>0.006352</td>\n",
       "      <td>0.014553</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.002103</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.002321</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.001478</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.002532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>hello Please can I book a boiler service. Home...</td>\n",
       "      <td>test-25</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.011614</td>\n",
       "      <td>0.025671</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>0.023084</td>\n",
       "      <td>0.002189</td>\n",
       "      <td>0.001578</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.000865</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.000818</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000398</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.002311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>the we missed the engineer coming today</td>\n",
       "      <td>test-26</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.020065</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.066786</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.015648</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001261</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.000487</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.001755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>i have a leaking radiator</td>\n",
       "      <td>test-27</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.040594</td>\n",
       "      <td>0.017727</td>\n",
       "      <td>0.006688</td>\n",
       "      <td>0.006254</td>\n",
       "      <td>0.003354</td>\n",
       "      <td>0.004130</td>\n",
       "      <td>0.003540</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002741</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.002172</td>\n",
       "      <td>0.000591</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.001389</td>\n",
       "      <td>0.002968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>my gas bill is not correct</td>\n",
       "      <td>test-28</td>\n",
       "      <td>0.157477</td>\n",
       "      <td>0.002561</td>\n",
       "      <td>0.003689</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.009235</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.000623</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.000443</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>my central heating's gone off_line</td>\n",
       "      <td>test-29</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>0.029242</td>\n",
       "      <td>0.017674</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.010214</td>\n",
       "      <td>0.005051</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.013812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.001642</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.004023</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.002585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4025</th>\n",
       "      <td>\"Our home services contract is due to renew on...</td>\n",
       "      <td>test-4025</td>\n",
       "      <td>0.004128</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.130071</td>\n",
       "      <td>0.004993</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000410</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000279</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000724</td>\n",
       "      <td>0.005805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4026</th>\n",
       "      <td>75 luton road lu54lw mr andrew windsor frost</td>\n",
       "      <td>test-4026</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.007194</td>\n",
       "      <td>0.004669</td>\n",
       "      <td>0.004452</td>\n",
       "      <td>0.010088</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.371472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000628</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.001137</td>\n",
       "      <td>0.003006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4027</th>\n",
       "      <td>the meter's not working</td>\n",
       "      <td>test-4027</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.033564</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.002765</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>0.010736</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.001253</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.001814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4028</th>\n",
       "      <td>Hi I’ve has a msg asking for my meter readings...</td>\n",
       "      <td>test-4028</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>0.008882</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.008326</td>\n",
       "      <td>0.001903</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.012032</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000421</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000434</td>\n",
       "      <td>0.001732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4029</th>\n",
       "      <td>\"Hi Elaine, I tried to call yesterday but got ...</td>\n",
       "      <td>test-4029</td>\n",
       "      <td>0.007984</td>\n",
       "      <td>0.003047</td>\n",
       "      <td>0.009247</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001501</td>\n",
       "      <td>0.002652</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.015334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000539</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.000448</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>0.001922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4030</th>\n",
       "      <td>to find out the cheapest tariffs</td>\n",
       "      <td>test-4030</td>\n",
       "      <td>0.027908</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.011598</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.001047</td>\n",
       "      <td>0.006966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000405</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.000278</td>\n",
       "      <td>0.000387</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000281</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.002211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4031</th>\n",
       "      <td>Hello I am trying to book an appointment onlin...</td>\n",
       "      <td>test-4031</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.017533</td>\n",
       "      <td>0.009705</td>\n",
       "      <td>0.003247</td>\n",
       "      <td>0.059946</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.010700</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.000619</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001940</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.002010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4032</th>\n",
       "      <td>i want to speak to an advisor please about my ...</td>\n",
       "      <td>test-4032</td>\n",
       "      <td>0.029404</td>\n",
       "      <td>0.002126</td>\n",
       "      <td>0.011486</td>\n",
       "      <td>0.001023</td>\n",
       "      <td>0.001697</td>\n",
       "      <td>0.001184</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.007130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000393</td>\n",
       "      <td>0.000188</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000275</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000270</td>\n",
       "      <td>0.000267</td>\n",
       "      <td>0.000438</td>\n",
       "      <td>0.002069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4033</th>\n",
       "      <td>i received a message</td>\n",
       "      <td>test-4033</td>\n",
       "      <td>0.005266</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.005691</td>\n",
       "      <td>0.007994</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.005987</td>\n",
       "      <td>0.021500</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001832</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.002539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4034</th>\n",
       "      <td>i'd like a power flush done on our brick house...</td>\n",
       "      <td>test-4034</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.004438</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.003391</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.104009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.000780</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.001039</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.000748</td>\n",
       "      <td>0.001055</td>\n",
       "      <td>0.001582</td>\n",
       "      <td>0.003213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4035</th>\n",
       "      <td>a payment bill</td>\n",
       "      <td>test-4035</td>\n",
       "      <td>0.016571</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.002808</td>\n",
       "      <td>0.005794</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.019904</td>\n",
       "      <td>0.010764</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.000562</td>\n",
       "      <td>0.001071</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001620</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001275</td>\n",
       "      <td>0.001674</td>\n",
       "      <td>0.003327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4036</th>\n",
       "      <td>Hi I have had no electric in my house now for ...</td>\n",
       "      <td>test-4036</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.007037</td>\n",
       "      <td>0.002874</td>\n",
       "      <td>0.004667</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.005477</td>\n",
       "      <td>0.075677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001974</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.003616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4037</th>\n",
       "      <td>cancellation of homecare renewal</td>\n",
       "      <td>test-4037</td>\n",
       "      <td>0.004591</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.048103</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.006889</td>\n",
       "      <td>0.006462</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.000408</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.006805</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4038</th>\n",
       "      <td>give a gas reading</td>\n",
       "      <td>test-4038</td>\n",
       "      <td>0.015603</td>\n",
       "      <td>0.012484</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.001513</td>\n",
       "      <td>0.014019</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.001631</td>\n",
       "      <td>0.007200</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.000750</td>\n",
       "      <td>0.000698</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.000665</td>\n",
       "      <td>0.001563</td>\n",
       "      <td>0.000388</td>\n",
       "      <td>0.000293</td>\n",
       "      <td>0.000482</td>\n",
       "      <td>0.001985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4039</th>\n",
       "      <td>install a new meter</td>\n",
       "      <td>test-4039</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.014152</td>\n",
       "      <td>0.001539</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.009569</td>\n",
       "      <td>0.007212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.002280</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001190</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4040</th>\n",
       "      <td>\"Hi there, my gas bill has recently changed du...</td>\n",
       "      <td>test-4040</td>\n",
       "      <td>0.003218</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.001303</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.011715</td>\n",
       "      <td>0.008397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000687</td>\n",
       "      <td>0.000696</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.000480</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.003550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4041</th>\n",
       "      <td>i need to speak to the gas care my boiler's no...</td>\n",
       "      <td>test-4041</td>\n",
       "      <td>0.006463</td>\n",
       "      <td>0.007098</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.001255</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>0.004155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000558</td>\n",
       "      <td>0.000456</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000508</td>\n",
       "      <td>0.000721</td>\n",
       "      <td>0.001420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4042</th>\n",
       "      <td>there was a text message</td>\n",
       "      <td>test-4042</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.014918</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.005553</td>\n",
       "      <td>0.007730</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.005835</td>\n",
       "      <td>0.019957</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.001467</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.001829</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.000787</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.002444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4043</th>\n",
       "      <td>it's a final bill for a deceased customer</td>\n",
       "      <td>test-4043</td>\n",
       "      <td>0.092138</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.003601</td>\n",
       "      <td>0.002510</td>\n",
       "      <td>0.002068</td>\n",
       "      <td>0.002754</td>\n",
       "      <td>0.004106</td>\n",
       "      <td>0.035722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000543</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000651</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.000685</td>\n",
       "      <td>0.001409</td>\n",
       "      <td>0.002852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4044</th>\n",
       "      <td>query an account</td>\n",
       "      <td>test-4044</td>\n",
       "      <td>0.013662</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.008853</td>\n",
       "      <td>0.002888</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.002259</td>\n",
       "      <td>0.006170</td>\n",
       "      <td>0.011383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.000490</td>\n",
       "      <td>0.000857</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.003126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4045</th>\n",
       "      <td>electrical repairs</td>\n",
       "      <td>test-4045</td>\n",
       "      <td>0.003490</td>\n",
       "      <td>0.022308</td>\n",
       "      <td>0.003345</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.006402</td>\n",
       "      <td>0.003239</td>\n",
       "      <td>0.003324</td>\n",
       "      <td>0.032493</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.001223</td>\n",
       "      <td>0.002008</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.001132</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.002388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4046</th>\n",
       "      <td>i had a letter from you because you were comin...</td>\n",
       "      <td>test-4046</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.015615</td>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.003903</td>\n",
       "      <td>0.014775</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.007479</td>\n",
       "      <td>0.009472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.002217</td>\n",
       "      <td>0.001443</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.001704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4047</th>\n",
       "      <td>i have no gas card</td>\n",
       "      <td>test-4047</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>0.003804</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.005432</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.079349</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001658</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001262</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.003908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4048</th>\n",
       "      <td>query on homecare</td>\n",
       "      <td>test-4048</td>\n",
       "      <td>0.003082</td>\n",
       "      <td>0.003608</td>\n",
       "      <td>0.678543</td>\n",
       "      <td>0.006572</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.001733</td>\n",
       "      <td>0.001602</td>\n",
       "      <td>0.008511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000909</td>\n",
       "      <td>0.000305</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.008342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4049</th>\n",
       "      <td>noisy pump</td>\n",
       "      <td>test-4049</td>\n",
       "      <td>0.003144</td>\n",
       "      <td>0.016630</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.004736</td>\n",
       "      <td>0.005825</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.062225</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001360</td>\n",
       "      <td>0.001682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4050</th>\n",
       "      <td>i want to speak to somebody about the final bill</td>\n",
       "      <td>test-4050</td>\n",
       "      <td>0.312350</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.003615</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.002020</td>\n",
       "      <td>0.001871</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.009845</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.000403</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.000423</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.000346</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.002306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4051</th>\n",
       "      <td>my central heating is going on when it shouldn...</td>\n",
       "      <td>test-4051</td>\n",
       "      <td>0.010035</td>\n",
       "      <td>0.020505</td>\n",
       "      <td>0.012245</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.011017</td>\n",
       "      <td>0.003727</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.011928</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.003405</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4052</th>\n",
       "      <td>My account information informs me the my smart...</td>\n",
       "      <td>test-4052</td>\n",
       "      <td>0.001309</td>\n",
       "      <td>0.011568</td>\n",
       "      <td>0.002675</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.008372</td>\n",
       "      <td>0.003429</td>\n",
       "      <td>0.005728</td>\n",
       "      <td>0.011276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001240</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.002069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4053</th>\n",
       "      <td>i've been asked to book my annual service</td>\n",
       "      <td>test-4053</td>\n",
       "      <td>0.003722</td>\n",
       "      <td>0.019006</td>\n",
       "      <td>0.010970</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.030375</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001369</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.000641</td>\n",
       "      <td>0.000668</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.001968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4054</th>\n",
       "      <td>give you your money</td>\n",
       "      <td>test-4054</td>\n",
       "      <td>0.012572</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.002376</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.001552</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.013368</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.000850</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001379</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.001056</td>\n",
       "      <td>0.002662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4055 rows × 252 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           comment_text         id  \\\n",
       "0                                      refund of excess     test-0   \n",
       "1                                       upset and angry     test-1   \n",
       "2                   to set_up a direct_debit for my gas     test-2   \n",
       "3     i've moved house and i just missed call from y...     test-3   \n",
       "4                                  we have a wrong bill     test-4   \n",
       "5                                i have a plumbing leak     test-5   \n",
       "6     Hi i just moved 2 days ago new home i dis swit...     test-6   \n",
       "7       i've got problem with my central heating boiler     test-7   \n",
       "8              i just received my bill and it's about a     test-8   \n",
       "9     Hi my free electric that I got on a Sunday has...     test-9   \n",
       "10                                    to end an account    test-10   \n",
       "11                              speak about my homecare    test-11   \n",
       "12    \"Hi lakshmi, i am moving house - the app would...    test-12   \n",
       "13                                      homecare cancel    test-13   \n",
       "14             regarding e-mail regarding meter reading    test-14   \n",
       "15    \"Hi, I cannot log into my account its not reco...    test-15   \n",
       "16    it's about my gas and electricity tariff that ...    test-16   \n",
       "17                                   hear a transaction    test-17   \n",
       "18               to give you a up to date meter reading    test-18   \n",
       "19                                         credit meter    test-19   \n",
       "20                       to amend my homecare agreement    test-20   \n",
       "21                         i want to cancel my contract    test-21   \n",
       "22    Hi sai Please can u send us the latest bill by...    test-22   \n",
       "23    incorrect name and address for an electricity ...    test-23   \n",
       "24                                          gas problem    test-24   \n",
       "25    hello Please can I book a boiler service. Home...    test-25   \n",
       "26              the we missed the engineer coming today    test-26   \n",
       "27                            i have a leaking radiator    test-27   \n",
       "28                           my gas bill is not correct    test-28   \n",
       "29                   my central heating's gone off_line    test-29   \n",
       "...                                                 ...        ...   \n",
       "4025  \"Our home services contract is due to renew on...  test-4025   \n",
       "4026       75 luton road lu54lw mr andrew windsor frost  test-4026   \n",
       "4027                            the meter's not working  test-4027   \n",
       "4028  Hi I’ve has a msg asking for my meter readings...  test-4028   \n",
       "4029  \"Hi Elaine, I tried to call yesterday but got ...  test-4029   \n",
       "4030                   to find out the cheapest tariffs  test-4030   \n",
       "4031  Hello I am trying to book an appointment onlin...  test-4031   \n",
       "4032  i want to speak to an advisor please about my ...  test-4032   \n",
       "4033                               i received a message  test-4033   \n",
       "4034  i'd like a power flush done on our brick house...  test-4034   \n",
       "4035                                     a payment bill  test-4035   \n",
       "4036  Hi I have had no electric in my house now for ...  test-4036   \n",
       "4037                   cancellation of homecare renewal  test-4037   \n",
       "4038                                 give a gas reading  test-4038   \n",
       "4039                                install a new meter  test-4039   \n",
       "4040  \"Hi there, my gas bill has recently changed du...  test-4040   \n",
       "4041  i need to speak to the gas care my boiler's no...  test-4041   \n",
       "4042                           there was a text message  test-4042   \n",
       "4043          it's a final bill for a deceased customer  test-4043   \n",
       "4044                                   query an account  test-4044   \n",
       "4045                                 electrical repairs  test-4045   \n",
       "4046  i had a letter from you because you were comin...  test-4046   \n",
       "4047                                 i have no gas card  test-4047   \n",
       "4048                                  query on homecare  test-4048   \n",
       "4049                                         noisy pump  test-4049   \n",
       "4050   i want to speak to somebody about the final bill  test-4050   \n",
       "4051  my central heating is going on when it shouldn...  test-4051   \n",
       "4052  My account information informs me the my smart...  test-4052   \n",
       "4053          i've been asked to book my annual service  test-4053   \n",
       "4054                                give you your money  test-4054   \n",
       "\n",
       "      enquire-bill  problem-hot_water  enquire-agreement  \\\n",
       "0         0.016918           0.001712           0.004922   \n",
       "1         0.010182           0.003531           0.003536   \n",
       "2         0.003219           0.001458           0.009117   \n",
       "3         0.012253           0.003399           0.005182   \n",
       "4         0.321525           0.001334           0.002469   \n",
       "5         0.001344           0.048212           0.009657   \n",
       "6         0.018381           0.004637           0.005581   \n",
       "7         0.006559           0.007286           0.003290   \n",
       "8         0.544706           0.001646           0.003378   \n",
       "9         0.023657           0.002069           0.015792   \n",
       "10        0.016380           0.001445           0.018733   \n",
       "11        0.003246           0.003496           0.679273   \n",
       "12        0.005333           0.002711           0.010009   \n",
       "13        0.004981           0.001742           0.054062   \n",
       "14        0.013042           0.012599           0.001878   \n",
       "15        0.007393           0.001828           0.015621   \n",
       "16        0.028830           0.002067           0.011123   \n",
       "17        0.022015           0.002023           0.003755   \n",
       "18        0.015410           0.012399           0.001850   \n",
       "19        0.002396           0.007094           0.001218   \n",
       "20        0.003090           0.002079           0.205809   \n",
       "21        0.004657           0.001920           0.072696   \n",
       "22        0.041587           0.001573           0.002699   \n",
       "23        0.053010           0.001334           0.007808   \n",
       "24        0.003420           0.026205           0.002521   \n",
       "25        0.002944           0.011614           0.025671   \n",
       "26        0.003560           0.020065           0.004702   \n",
       "27        0.001670           0.040594           0.017727   \n",
       "28        0.157477           0.002561           0.003689   \n",
       "29        0.004560           0.029242           0.017674   \n",
       "...            ...                ...                ...   \n",
       "4025      0.004128           0.002323           0.130071   \n",
       "4026      0.007533           0.007194           0.004669   \n",
       "4027      0.001912           0.033564           0.001514   \n",
       "4028      0.033808           0.008882           0.001948   \n",
       "4029      0.007984           0.003047           0.009247   \n",
       "4030      0.027908           0.002078           0.011598   \n",
       "4031      0.003854           0.017533           0.009705   \n",
       "4032      0.029404           0.002126           0.011486   \n",
       "4033      0.005266           0.014386           0.001768   \n",
       "4034      0.003469           0.005425           0.004438   \n",
       "4035      0.016571           0.002368           0.002808   \n",
       "4036      0.002586           0.007037           0.002874   \n",
       "4037      0.004591           0.001797           0.048103   \n",
       "4038      0.015603           0.012484           0.001818   \n",
       "4039      0.001118           0.014152           0.001539   \n",
       "4040      0.003218           0.001290           0.005894   \n",
       "4041      0.006463           0.007098           0.003168   \n",
       "4042      0.005260           0.014918           0.001710   \n",
       "4043      0.092138           0.001093           0.003601   \n",
       "4044      0.013662           0.000872           0.008853   \n",
       "4045      0.003490           0.022308           0.003345   \n",
       "4046      0.001345           0.015615           0.001720   \n",
       "4047      0.002631           0.003804           0.002704   \n",
       "4048      0.003082           0.003608           0.678543   \n",
       "4049      0.003144           0.016630           0.002148   \n",
       "4050      0.312350           0.001298           0.003615   \n",
       "4051      0.010035           0.020505           0.012245   \n",
       "4052      0.001309           0.011568           0.002675   \n",
       "4053      0.003722           0.019006           0.010970   \n",
       "4054      0.012572           0.002146           0.002376   \n",
       "\n",
       "      enquire-safety_certificate  enquire-appointment_when  enquire-heating  \\\n",
       "0                       0.002933                  0.001359         0.001845   \n",
       "1                       0.004904                  0.004838         0.002393   \n",
       "2                       0.003295                  0.001608         0.002058   \n",
       "3                       0.001347                  0.001661         0.002313   \n",
       "4                       0.001510                  0.002236         0.001428   \n",
       "5                       0.006983                  0.008075         0.002964   \n",
       "6                       0.001341                  0.002007         0.002491   \n",
       "7                       0.001439                  0.003096         0.001331   \n",
       "8                       0.001187                  0.002552         0.001237   \n",
       "9                       0.001141                  0.001812         0.001270   \n",
       "10                      0.004736                  0.001888         0.002913   \n",
       "11                      0.006488                  0.002154         0.001733   \n",
       "12                      0.002747                  0.001680         0.003351   \n",
       "13                      0.007985                  0.001863         0.001903   \n",
       "14                      0.001548                  0.014834         0.001537   \n",
       "15                      0.003940                  0.001607         0.002716   \n",
       "16                      0.000955                  0.001651         0.001195   \n",
       "17                      0.004643                  0.002839         0.002563   \n",
       "18                      0.001510                  0.014121         0.001629   \n",
       "19                      0.003851                  0.005842         0.002120   \n",
       "20                      0.006085                  0.001311         0.001903   \n",
       "21                      0.008365                  0.001978         0.001822   \n",
       "22                      0.004127                  0.002769         0.002619   \n",
       "23                      0.003507                  0.001990         0.002925   \n",
       "24                      0.005462                  0.005787         0.003637   \n",
       "25                      0.003199                  0.023084         0.002189   \n",
       "26                      0.003048                  0.066786         0.001621   \n",
       "27                      0.006688                  0.006254         0.003354   \n",
       "28                      0.001486                  0.002199         0.001774   \n",
       "29                      0.003704                  0.010214         0.005051   \n",
       "...                          ...                       ...              ...   \n",
       "4025                    0.004993                  0.001268         0.001420   \n",
       "4026                    0.004452                  0.010088         0.002447   \n",
       "4027                    0.002810                  0.010641         0.002765   \n",
       "4028                    0.001322                  0.008326         0.001903   \n",
       "4029                    0.001787                  0.001501         0.002652   \n",
       "4030                    0.000982                  0.001686         0.001221   \n",
       "4031                    0.003247                  0.059946         0.002146   \n",
       "4032                    0.001023                  0.001697         0.001184   \n",
       "4033                    0.005691                  0.007994         0.002108   \n",
       "4034                    0.004762                  0.003305         0.003391   \n",
       "4035                    0.005794                  0.001698         0.001781   \n",
       "4036                    0.004667                  0.004255         0.004759   \n",
       "4037                    0.006800                  0.001608         0.001684   \n",
       "4038                    0.001513                  0.014019         0.001628   \n",
       "4039                    0.004242                  0.011344         0.002751   \n",
       "4040                    0.003049                  0.001303         0.001825   \n",
       "4041                    0.001396                  0.002973         0.001255   \n",
       "4042                    0.005553                  0.007730         0.002046   \n",
       "4043                    0.002510                  0.002068         0.002754   \n",
       "4044                    0.002888                  0.001398         0.002259   \n",
       "4045                    0.005499                  0.006402         0.003239   \n",
       "4046                    0.003903                  0.014775         0.002723   \n",
       "4047                    0.005432                  0.003027         0.002125   \n",
       "4048                    0.006572                  0.002269         0.001733   \n",
       "4049                    0.004736                  0.005825         0.002234   \n",
       "4050                    0.001459                  0.002020         0.001871   \n",
       "4051                    0.002439                  0.011017         0.003727   \n",
       "4052                    0.003350                  0.008372         0.003429   \n",
       "4053                    0.002827                  0.030375         0.002387   \n",
       "4054                    0.004327                  0.001552         0.001413   \n",
       "\n",
       "      request-card  enquire-vague          ...            enquire-hive_sensor  \\\n",
       "0         0.009211       0.007514          ...                       0.000736   \n",
       "1         0.003865       0.370754          ...                       0.001380   \n",
       "2         0.011558       0.010287          ...                       0.000780   \n",
       "3         0.001019       0.009998          ...                       0.000552   \n",
       "4         0.002454       0.012880          ...                       0.000964   \n",
       "5         0.006388       0.003383          ...                       0.003234   \n",
       "6         0.001117       0.010923          ...                       0.000590   \n",
       "7         0.001720       0.004138          ...                       0.000744   \n",
       "8         0.001665       0.006385          ...                       0.000881   \n",
       "9         0.001033       0.008237          ...                       0.000424   \n",
       "10        0.004289       0.025088          ...                       0.002315   \n",
       "11        0.001629       0.008706          ...                       0.001213   \n",
       "12        0.002173       0.016839          ...                       0.001159   \n",
       "13        0.007878       0.007157          ...                       0.001437   \n",
       "14        0.001504       0.007197          ...                       0.000661   \n",
       "15        0.003541       0.024591          ...                       0.001716   \n",
       "16        0.001033       0.006737          ...                       0.000400   \n",
       "17        0.006636       0.062644          ...                       0.001226   \n",
       "18        0.001616       0.007310          ...                       0.000683   \n",
       "19        0.015475       0.014025          ...                       0.001151   \n",
       "20        0.002828       0.009035          ...                       0.000889   \n",
       "21        0.007292       0.006895          ...                       0.001326   \n",
       "22        0.004497       0.065462          ...                       0.001756   \n",
       "23        0.003732       0.030908          ...                       0.001957   \n",
       "24        0.006352       0.014553          ...                       0.001913   \n",
       "25        0.001578       0.005666          ...                       0.001327   \n",
       "26        0.002054       0.015648          ...                       0.001261   \n",
       "27        0.004130       0.003540          ...                       0.002741   \n",
       "28        0.002046       0.009235          ...                       0.001096   \n",
       "29        0.002171       0.013812          ...                       0.001521   \n",
       "...            ...            ...          ...                            ...   \n",
       "4025      0.003488       0.006364          ...                       0.000707   \n",
       "4026      0.002354       0.371472          ...                       0.001357   \n",
       "4027      0.005684       0.010736          ...                       0.001092   \n",
       "4028      0.001296       0.012032          ...                       0.000632   \n",
       "4029      0.001113       0.015334          ...                       0.000727   \n",
       "4030      0.001047       0.006966          ...                       0.000405   \n",
       "4031      0.001658       0.010700          ...                       0.001451   \n",
       "4032      0.000974       0.007130          ...                       0.000393   \n",
       "4033      0.005987       0.021500          ...                       0.001784   \n",
       "4034      0.002947       0.104009          ...                       0.001154   \n",
       "4035      0.019904       0.010764          ...                       0.001310   \n",
       "4036      0.005477       0.075677          ...                       0.001246   \n",
       "4037      0.006889       0.006462          ...                       0.001212   \n",
       "4038      0.001631       0.007200          ...                       0.000689   \n",
       "4039      0.009569       0.007212          ...                       0.001601   \n",
       "4040      0.011715       0.008397          ...                       0.000687   \n",
       "4041      0.001661       0.004155          ...                       0.000714   \n",
       "4042      0.005835       0.019957          ...                       0.001735   \n",
       "4043      0.004106       0.035722          ...                       0.001373   \n",
       "4044      0.006170       0.011383          ...                       0.001672   \n",
       "4045      0.003324       0.032493          ...                       0.001471   \n",
       "4046      0.007479       0.009472          ...                       0.001528   \n",
       "4047      0.079349       0.003155          ...                       0.001658   \n",
       "4048      0.001602       0.008511          ...                       0.001218   \n",
       "4049      0.002718       0.062225          ...                       0.001218   \n",
       "4050      0.002791       0.009845          ...                       0.000930   \n",
       "4051      0.001727       0.011928          ...                       0.001178   \n",
       "4052      0.005728       0.011276          ...                       0.001240   \n",
       "4053      0.001441       0.005777          ...                       0.001369   \n",
       "4054      0.013368       0.009712          ...                       0.000875   \n",
       "\n",
       "      enquire-hive_bulb  enquire-hive_thermostat  enquire-hive_app  \\\n",
       "0              0.000746                 0.000646          0.000936   \n",
       "1              0.000658                 0.000813          0.001199   \n",
       "2              0.000775                 0.000506          0.000506   \n",
       "3              0.000561                 0.000585          0.000651   \n",
       "4              0.000388                 0.000548          0.000780   \n",
       "5              0.001428                 0.002377          0.000846   \n",
       "6              0.000633                 0.000658          0.000766   \n",
       "7              0.000401                 0.000591          0.000479   \n",
       "8              0.000323                 0.000567          0.000592   \n",
       "9              0.000193                 0.000292          0.000383   \n",
       "10             0.000669                 0.000686          0.001419   \n",
       "11             0.000530                 0.000624          0.000582   \n",
       "12             0.000800                 0.000775          0.001161   \n",
       "13             0.000579                 0.001057          0.000667   \n",
       "14             0.000670                 0.000713          0.000519   \n",
       "15             0.000631                 0.000606          0.000993   \n",
       "16             0.000186                 0.000269          0.000348   \n",
       "17             0.000754                 0.000916          0.001302   \n",
       "18             0.000746                 0.000694          0.000540   \n",
       "19             0.001087                 0.001001          0.001176   \n",
       "20             0.000445                 0.000520          0.000530   \n",
       "21             0.000564                 0.001072          0.000637   \n",
       "22             0.000650                 0.000813          0.001315   \n",
       "23             0.000604                 0.000674          0.001714   \n",
       "24             0.000741                 0.002103          0.001119   \n",
       "25             0.000543                 0.000865          0.000605   \n",
       "26             0.000709                 0.001017          0.000574   \n",
       "27             0.001141                 0.001784          0.000721   \n",
       "28             0.000418                 0.000607          0.000607   \n",
       "29             0.001055                 0.001642          0.000795   \n",
       "...                 ...                      ...               ...   \n",
       "4025           0.000335                 0.000532          0.000410   \n",
       "4026           0.000656                 0.001057          0.000930   \n",
       "4027           0.001094                 0.001163          0.000910   \n",
       "4028           0.000743                 0.000639          0.000672   \n",
       "4029           0.000539                 0.000566          0.000685   \n",
       "4030           0.000188                 0.000273          0.000352   \n",
       "4031           0.000619                 0.000991          0.000642   \n",
       "4032           0.000188                 0.000270          0.000350   \n",
       "4033           0.001230                 0.001481          0.001020   \n",
       "4034           0.000780                 0.000915          0.001039   \n",
       "4035           0.000562                 0.001071          0.001015   \n",
       "4036           0.001073                 0.001099          0.001122   \n",
       "4037           0.000512                 0.000906          0.000573   \n",
       "4038           0.000750                 0.000698          0.000544   \n",
       "4039           0.001468                 0.001729          0.000940   \n",
       "4040           0.000696                 0.000449          0.000480   \n",
       "4041           0.000372                 0.000558          0.000456   \n",
       "4042           0.001199                 0.001467          0.001008   \n",
       "4043           0.000588                 0.000543          0.001671   \n",
       "4044           0.000481                 0.000469          0.000948   \n",
       "4045           0.000788                 0.001721          0.000834   \n",
       "4046           0.001346                 0.001439          0.000955   \n",
       "4047           0.001384                 0.001593          0.001177   \n",
       "4048           0.000542                 0.000635          0.000584   \n",
       "4049           0.000624                 0.001097          0.000731   \n",
       "4050           0.000403                 0.000411          0.000882   \n",
       "4051           0.000850                 0.001382          0.000723   \n",
       "4052           0.001576                 0.001381          0.001037   \n",
       "4053           0.000552                 0.000944          0.000641   \n",
       "4054           0.000502                 0.000810          0.000850   \n",
       "\n",
       "      enquire-install_hive  problem-hive_thermostat  cancel-hive  \\\n",
       "0                 0.000776                 0.001720     0.000737   \n",
       "1                 0.000664                 0.001722     0.000753   \n",
       "2                 0.000625                 0.000927     0.000531   \n",
       "3                 0.000766                 0.001269     0.000425   \n",
       "4                 0.000397                 0.001063     0.000423   \n",
       "5                 0.002067                 0.002179     0.000817   \n",
       "6                 0.000846                 0.001479     0.000435   \n",
       "7                 0.000737                 0.001135     0.000202   \n",
       "8                 0.000369                 0.000801     0.000308   \n",
       "9                 0.000281                 0.000395     0.000277   \n",
       "10                0.000869                 0.001698     0.000721   \n",
       "11                0.000699                 0.000881     0.000297   \n",
       "12                0.001024                 0.001782     0.000607   \n",
       "13                0.001154                 0.000953     0.000435   \n",
       "14                0.000613                 0.001519     0.000377   \n",
       "15                0.000857                 0.001717     0.000449   \n",
       "16                0.000276                 0.000387     0.000265   \n",
       "17                0.000842                 0.001815     0.001057   \n",
       "18                0.000658                 0.001553     0.000388   \n",
       "19                0.001125                 0.001949     0.001384   \n",
       "20                0.000738                 0.000844     0.000327   \n",
       "21                0.001191                 0.000926     0.000414   \n",
       "22                0.000705                 0.001866     0.000804   \n",
       "23                0.001015                 0.001824     0.000995   \n",
       "24                0.001549                 0.002321     0.001295   \n",
       "25                0.000818                 0.001484     0.000398   \n",
       "26                0.000487                 0.001902     0.000900   \n",
       "27                0.001997                 0.002172     0.000591   \n",
       "28                0.000623                 0.001014     0.000443   \n",
       "29                0.001284                 0.004023     0.000913   \n",
       "...                    ...                      ...          ...   \n",
       "4025              0.000790                 0.000717     0.000279   \n",
       "4026              0.000628                 0.001588     0.000743   \n",
       "4027              0.001103                 0.001935     0.001253   \n",
       "4028              0.000610                 0.001290     0.000421   \n",
       "4029              0.000702                 0.001260     0.000448   \n",
       "4030              0.000278                 0.000387     0.000270   \n",
       "4031              0.000562                 0.001940     0.000622   \n",
       "4032              0.000275                 0.000372     0.000270   \n",
       "4033              0.000906                 0.001832     0.001123   \n",
       "4034              0.000957                 0.001902     0.000748   \n",
       "4035              0.001185                 0.001620     0.001007   \n",
       "4036              0.001317                 0.001974     0.001164   \n",
       "4037              0.001026                 0.000846     0.000408   \n",
       "4038              0.000665                 0.001563     0.000388   \n",
       "4039              0.001301                 0.002280     0.001382   \n",
       "4040              0.000595                 0.000882     0.000450   \n",
       "4041              0.000681                 0.001042     0.000190   \n",
       "4042              0.000909                 0.001829     0.001132   \n",
       "4043              0.000651                 0.001718     0.000991   \n",
       "4044              0.000661                 0.001042     0.000490   \n",
       "4045              0.001223                 0.002008     0.000997   \n",
       "4046              0.001037                 0.002217     0.001443   \n",
       "4047              0.001630                 0.001733     0.000891   \n",
       "4048              0.000701                 0.000909     0.000305   \n",
       "4049              0.000886                 0.001519     0.000736   \n",
       "4050              0.000423                 0.001052     0.000503   \n",
       "4051              0.001013                 0.003405     0.000611   \n",
       "4052              0.001352                 0.002518     0.001211   \n",
       "4053              0.000668                 0.001729     0.000469   \n",
       "4054              0.000885                 0.001379     0.000842   \n",
       "\n",
       "      problem-hive_sensor  problem-hive_camera  enquire-business_care  \n",
       "0                0.000615             0.000937               0.003488  \n",
       "1                0.001121             0.001442               0.003300  \n",
       "2                0.000855             0.000750               0.004111  \n",
       "3                0.000473             0.000681               0.001522  \n",
       "4                0.000338             0.000677               0.001798  \n",
       "5                0.001519             0.001613               0.002879  \n",
       "6                0.000472             0.000733               0.001778  \n",
       "7                0.000543             0.000772               0.001464  \n",
       "8                0.000248             0.000528               0.001654  \n",
       "9                0.000309             0.000450               0.002390  \n",
       "10               0.001189             0.001173               0.004840  \n",
       "11               0.000752             0.000613               0.008596  \n",
       "12               0.000970             0.001016               0.002533  \n",
       "13               0.001088             0.000993               0.007769  \n",
       "14               0.000274             0.000459               0.001884  \n",
       "15               0.001148             0.001172               0.003586  \n",
       "16               0.000272             0.000430               0.002180  \n",
       "17               0.001095             0.001735               0.003599  \n",
       "18               0.000292             0.000478               0.001977  \n",
       "19               0.000946             0.001238               0.002387  \n",
       "20               0.000843             0.000766               0.006822  \n",
       "21               0.001039             0.000986               0.008324  \n",
       "22               0.001167             0.001536               0.002783  \n",
       "23               0.001013             0.001456               0.003039  \n",
       "24               0.001478             0.002287               0.002532  \n",
       "25               0.000660             0.000774               0.002311  \n",
       "26               0.000562             0.000710               0.001755  \n",
       "27               0.001325             0.001389               0.002968  \n",
       "28               0.000566             0.000834               0.001888  \n",
       "29               0.001315             0.001216               0.002585  \n",
       "...                   ...                  ...                    ...  \n",
       "4025             0.000707             0.000724               0.005805  \n",
       "4026             0.000771             0.001137               0.003006  \n",
       "4027             0.000848             0.001162               0.001814  \n",
       "4028             0.000342             0.000434               0.001732  \n",
       "4029             0.000571             0.000754               0.001922  \n",
       "4030             0.000281             0.000435               0.002211  \n",
       "4031             0.000688             0.000727               0.002010  \n",
       "4032             0.000267             0.000438               0.002069  \n",
       "4033             0.000820             0.001375               0.002539  \n",
       "4034             0.001055             0.001582               0.003213  \n",
       "4035             0.001275             0.001674               0.003327  \n",
       "4036             0.001672             0.001979               0.003616  \n",
       "4037             0.001000             0.000910               0.006805  \n",
       "4038             0.000293             0.000482               0.001985  \n",
       "4039             0.001190             0.001390               0.001660  \n",
       "4040             0.000744             0.000639               0.003550  \n",
       "4041             0.000508             0.000721               0.001420  \n",
       "4042             0.000787             0.001350               0.002444  \n",
       "4043             0.000685             0.001409               0.002852  \n",
       "4044             0.000857             0.000828               0.003126  \n",
       "4045             0.001132             0.001731               0.002388  \n",
       "4046             0.001115             0.001336               0.001704  \n",
       "4047             0.001262             0.001256               0.003908  \n",
       "4048             0.000758             0.000612               0.008342  \n",
       "4049             0.000702             0.001360               0.001682  \n",
       "4050             0.000346             0.000717               0.002306  \n",
       "4051             0.000936             0.001073               0.002387  \n",
       "4052             0.001373             0.001226               0.002069  \n",
       "4053             0.000588             0.000699               0.001968  \n",
       "4054             0.000715             0.001056               0.002662  \n",
       "\n",
       "[4055 rows x 252 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(enquire-payment          0.103659\n",
       " enquire-refund          0.0833599\n",
       " pay-bill                 0.071568\n",
       " enquire-direct_debit    0.0189417\n",
       " pay-bill_energy         0.0177325\n",
       " Name: 4054, dtype: object, 'give you your money')"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = 4054\n",
    "result.iloc[row][2:-1].sort_values(ascending=False)[:5], result.iloc[row]['comment_text'],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('cancel-contract', 0.4749981),\n",
       "  ('renew-agreement', 0.11200493),\n",
       "  ('enquire-complaint', 0.052506145),\n",
       "  ('enquire-agreement', 0.046284176),\n",
       "  ('cancel-energy', 0.020538487),\n",
       "  ('pay-bill', 0.02034559),\n",
       "  ('enquire-account', 0.018070748),\n",
       "  ('enquire-account_online', 0.016787473),\n",
       "  ('enquire-payment', 0.016535241),\n",
       "  ('setup-account', 0.014415508),\n",
       "  ('cancel-appointment', 0.013989136),\n",
       "  ('change-homecare', 0.013885322),\n",
       "  ('change-name', 0.012765076),\n",
       "  ('enquire-card', 0.012473138),\n",
       "  ('enquire-tariff', 0.012216496),\n",
       "  ('change-address', 0.011427024),\n",
       "  ('enquire-refund', 0.01102401),\n",
       "  ('change-supplier', 0.010753986),\n",
       "  ('problem-card', 0.010590665),\n",
       "  ('change-email', 0.010466893),\n",
       "  ('enquire-direct_debit', 0.010154515),\n",
       "  ('change-direct_debit', 0.009339089),\n",
       "  ('enquire-debt', 0.009233614),\n",
       "  ('enquire-balance', 0.0087736165),\n",
       "  ('request-card', 0.008220891),\n",
       "  ('enquire-business_care', 0.0078898715),\n",
       "  ('enquire-safety_certificate', 0.007807241),\n",
       "  ('pay-payment', 0.0077596996),\n",
       "  ('enquire-vague', 0.0076501663),\n",
       "  ('problem-direct_debit', 0.007626199),\n",
       "  ('problem-vague', 0.007444936),\n",
       "  ('setup-direct_debit', 0.006812724),\n",
       "  ('enquire-credit', 0.006558723),\n",
       "  ('enquire-quote', 0.006524264),\n",
       "  ('pay-bill_energy', 0.0061914935),\n",
       "  ('enquire-key', 0.0061350646),\n",
       "  ('cancel-vague', 0.006053335),\n",
       "  ('report-bereavement', 0.005852374),\n",
       "  ('report-payment_difficulty', 0.005777972),\n",
       "  ('enquire-payment_plan', 0.0057768575),\n",
       "  ('report-home_move', 0.005706037),\n",
       "  ('change-account_detail', 0.005631736),\n",
       "  ('problem-payment', 0.0055802427),\n",
       "  ('join-homecare', 0.0054805325),\n",
       "  ('enquire-bill', 0.005366647),\n",
       "  ('join-energy', 0.0051998678),\n",
       "  ('report-leak_heating', 0.005182227),\n",
       "  ('change-payment', 0.0051681483),\n",
       "  ('enquire-warm_home_discount', 0.0049132984),\n",
       "  ('enquire-password', 0.0046844673),\n",
       "  ('vague-top_up', 0.0046550524),\n",
       "  ('enquire-energy', 0.004526517),\n",
       "  ('enquire-insurance', 0.0044118976),\n",
       "  ('vague-compensation', 0.004260468),\n",
       "  ('report-card_lost', 0.004250381),\n",
       "  ('dispute-bill', 0.0042222105),\n",
       "  ('enquire-connection', 0.0041855737),\n",
       "  ('vague-breakdown', 0.00417128),\n",
       "  ('enquire-bill_homecare', 0.004148205),\n",
       "  ('enquire-bill_due', 0.004119007),\n",
       "  ('change-appointment_service', 0.00410274),\n",
       "  ('enquire-supplier', 0.0039938143),\n",
       "  ('enquire-bill_energy', 0.003922697),\n",
       "  ('request-bill', 0.003863923),\n",
       "  ('problem-boiler', 0.0038472172),\n",
       "  ('enquire-hive', 0.0038056765),\n",
       "  ('enquire-account_confirm', 0.0037362755),\n",
       "  ('enquire-bill_final', 0.0037292284),\n",
       "  ('setup-payment_plan', 0.0036196879),\n",
       "  ('problem-account', 0.0036039983),\n",
       "  ('problem-hive', 0.003600589),\n",
       "  ('problem-key', 0.0035808163),\n",
       "  ('problem-toilet', 0.0035621976),\n",
       "  ('report-appointment_missed', 0.0035129352),\n",
       "  ('report-bill_unpaid', 0.0035095934),\n",
       "  ('change-vague', 0.003504682),\n",
       "  ('report-contact', 0.003430644),\n",
       "  ('problem-bill', 0.0033728916),\n",
       "  ('report-key_lost', 0.0033515699),\n",
       "  ('request-bill_paper', 0.00318239),\n",
       "  ('enquire-voucher', 0.0031026113),\n",
       "  ('pay-bill_final', 0.003012521),\n",
       "  ('setup-hive', 0.0030072238),\n",
       "  ('enquire-price', 0.0029656854),\n",
       "  ('enquire-usage', 0.002931362),\n",
       "  ('problem-meter_payge', 0.0028129697),\n",
       "  ('report-bill_paid', 0.0028084994),\n",
       "  ('change-ownership', 0.0027868396),\n",
       "  ('problem-appliance', 0.0027664774),\n",
       "  ('report-direct_debit_changed', 0.0027663433),\n",
       "  ('cancel-direct_debit', 0.002722435),\n",
       "  ('report-property_new', 0.0027204612),\n",
       "  ('enquire-appointment_today', 0.0027041899),\n",
       "  ('report-meter_reading_final', 0.0027039638),\n",
       "  ('problem-gas', 0.0026492444),\n",
       "  ('report-leak_water', 0.0026248107),\n",
       "  ('enquire-statement', 0.0026146402),\n",
       "  ('enquire-meter_exchange', 0.0025511163),\n",
       "  ('vague-appliance', 0.0025395402),\n",
       "  ('request-balance', 0.0025183246),\n",
       "  ('enquire-business_account', 0.0024895938),\n",
       "  ('change-install_smart_meter', 0.0024602297),\n",
       "  ('report-no_energy', 0.0024407266),\n",
       "  ('enquire-meter_payge', 0.0024153683),\n",
       "  ('request-appointment_boiler', 0.0023248629),\n",
       "  ('report-letter', 0.0023131513),\n",
       "  ('pay-homecare', 0.0022678562),\n",
       "  ('enquire-repair', 0.0022527687),\n",
       "  ('enquire-invoice', 0.0022390522),\n",
       "  ('enquire-smart_meter', 0.0022102683),\n",
       "  ('enquire-meter', 0.0021986242),\n",
       "  ('cancel-smart_meter', 0.002190732),\n",
       "  ('setup-contract', 0.0021890982),\n",
       "  ('enquire-debt_meter', 0.002172916),\n",
       "  ('enquire-power_cut', 0.0021377443),\n",
       "  ('problem-bill_energy', 0.0021323815),\n",
       "  ('change-bill', 0.002112214),\n",
       "  ('enquire-gas_fire', 0.0021108184),\n",
       "  ('enquire-install', 0.002089487),\n",
       "  ('enquire-gas_safety', 0.0020547581),\n",
       "  ('enquire-bill_paid', 0.0020428728),\n",
       "  ('change-appointment_boiler', 0.0020372237),\n",
       "  ('problem-drain', 0.0020351862),\n",
       "  ('enquire-nectar', 0.0020257218),\n",
       "  ('report-leak', 0.0020233036),\n",
       "  ('problem-plumbing', 0.0019709922),\n",
       "  ('problem-hive_hub', 0.001959968),\n",
       "  ('report-cut_off', 0.001953547),\n",
       "  ('enquire-install_meter', 0.0019322652),\n",
       "  ('problem-heating', 0.0019189526),\n",
       "  ('enquire-heating', 0.001910935),\n",
       "  ('request-smart_meter', 0.0019054001),\n",
       "  ('report-blockage', 0.0018753364),\n",
       "  ('enquire-appointment_when', 0.0018750037),\n",
       "  ('enquire-activation_code', 0.0018716948),\n",
       "  ('enquire-power_flush', 0.0018328032),\n",
       "  ('problem-electric', 0.001820989),\n",
       "  ('change-appointment', 0.001795678),\n",
       "  ('register-card', 0.0017591765),\n",
       "  ('enquire-appointment_boiler', 0.0017570214),\n",
       "  ('request-meter_move', 0.0017367185),\n",
       "  ('problem-hot_water', 0.0017320614),\n",
       "  ('enquire-boiler_new', 0.0017199364),\n",
       "  ('request-advisor', 0.0017015565),\n",
       "  ('vague-plumbing', 0.0017013556),\n",
       "  ('report-meter_reading_opening', 0.0016966034),\n",
       "  ('report-leak_sink', 0.0016933887),\n",
       "  ('enquire-meter_reading', 0.0016511941),\n",
       "  ('request-annual_service', 0.0016472631),\n",
       "  ('enquire-service', 0.0016420464),\n",
       "  ('problem-smart_meter', 0.0016381035),\n",
       "  ('enquire-appointment_smart_meter', 0.001633194),\n",
       "  ('report-leak_gas', 0.0016237415),\n",
       "  ('problem-account_setup', 0.0016057808),\n",
       "  ('enquire-install_boiler', 0.0015996014),\n",
       "  ('problem-hive_heating', 0.0015748012),\n",
       "  ('enquire-annual_service', 0.0015658874),\n",
       "  ('enquire-charge', 0.0015656092),\n",
       "  ('enquire-bill_estimate', 0.0015554598),\n",
       "  ('enquire-claim', 0.0015299035),\n",
       "  ('request-install_meter_payge', 0.0015161472),\n",
       "  ('enquire-customer_number', 0.0015129605),\n",
       "  ('enquire-heating_upgrade', 0.0015103228),\n",
       "  ('enquire-appointment', 0.0015058912),\n",
       "  ('request-appointment_plumbing', 0.0014844057),\n",
       "  ('request-install_meter', 0.0014717171),\n",
       "  ('enquire-hive_sensor', 0.0014656878),\n",
       "  ('problem-shower', 0.0014445824),\n",
       "  ('request-service', 0.0014198228),\n",
       "  ('report-letter_leaving', 0.001418389),\n",
       "  ('request-meter_remove', 0.0014145054),\n",
       "  ('report-direct_debit_unpaid', 0.0014029412),\n",
       "  ('problem-water', 0.0014028744),\n",
       "  ('vague-fraud', 0.0013933928),\n",
       "  ('report-gas_smell', 0.0013772183),\n",
       "  ('enquire-meter_number', 0.0013740073),\n",
       "  ('problem-meter', 0.0013575688),\n",
       "  ('enquire-install_smart_meter', 0.0013547582),\n",
       "  ('enquire-bill_reminder', 0.0013546222),\n",
       "  ('enquire-charge_late_payment', 0.0013453349),\n",
       "  ('report-bill_high', 0.0013437137),\n",
       "  ('cancel-tariff', 0.001330626),\n",
       "  ('problem-pipe', 0.0012978838),\n",
       "  ('problem-boiler_pressure', 0.0012738226),\n",
       "  ('enquire-emergency_credit', 0.0012697877),\n",
       "  ('problem-home_move', 0.0012453682),\n",
       "  ('problem-meter_reading', 0.0012368207),\n",
       "  ('enquire-smart_card', 0.0012296494),\n",
       "  ('cancel-business_account', 0.0012229781),\n",
       "  ('report-house_sale', 0.0012145268),\n",
       "  ('request-appointment', 0.0011709852),\n",
       "  ('request-service_report', 0.0011682643),\n",
       "  ('report-gas_emergency', 0.0011567986),\n",
       "  ('enquire-install_hive', 0.0011481007),\n",
       "  ('change-payment_plan', 0.0011406357),\n",
       "  ('enquire-warranty', 0.0011382814),\n",
       "  ('enquire-hive_camera', 0.0011364466),\n",
       "  ('enquire-insulation', 0.001112423),\n",
       "  ('problem-hive_sensor', 0.0011081505),\n",
       "  ('problem-hive_app', 0.0011051608),\n",
       "  ('request-dyno_rod', 0.0011003966),\n",
       "  ('enquire-appointment_plumbing', 0.0010713296),\n",
       "  ('report-meter_reading_wrong', 0.0010707781),\n",
       "  ('enquire-property', 0.0010629366),\n",
       "  ('change-phone', 0.001062487),\n",
       "  ('enquire-account_overdue', 0.0010482506),\n",
       "  ('problem-hive_camera', 0.0010443332),\n",
       "  ('report-emergency', 0.0010425235),\n",
       "  ('enquire-hive_thermostat', 0.0010378279),\n",
       "  ('request-appointment_heating', 0.0010355775),\n",
       "  ('vague-landlord', 0.0010197714),\n",
       "  ('enquire-meter_reset', 0.001010595),\n",
       "  ('vague-solar_panel', 0.0009949849),\n",
       "  ('vague-business', 0.0009741301),\n",
       "  ('problem-hive_thermostat', 0.0009729364),\n",
       "  ('enquire-boiler_iq', 0.00097214425),\n",
       "  ('problem-payment_plan', 0.00096102373),\n",
       "  ('enquire-hive_hub', 0.00095892296),\n",
       "  ('enquire-call_back', 0.00095393934),\n",
       "  ('request-appointment_homecare', 0.00094037654),\n",
       "  ('request-appointment_meter_reading', 0.0009266938),\n",
       "  ('report-business_move', 0.0009248631),\n",
       "  ('enquire-gas_pipe', 0.00091835804),\n",
       "  ('enquire-install_heating', 0.00090677396),\n",
       "  ('vague-carbon_monoxide', 0.00089121674),\n",
       "  ('enquire-order', 0.00088663114),\n",
       "  ('request-invoice', 0.0008592853),\n",
       "  ('request-appointment_electric', 0.00083336775),\n",
       "  ('request-appointment_emergency', 0.0008051225),\n",
       "  ('enquire-hive_heating', 0.0007807268),\n",
       "  ('enquire-authority', 0.0007746557),\n",
       "  ('request-quote_energy', 0.0007617366),\n",
       "  ('enquire-bulb', 0.0007406013),\n",
       "  ('report-leak_drain', 0.0007268172),\n",
       "  ('report-meter_reading', 0.0007094192),\n",
       "  ('enquire-hive_app', 0.0007087768),\n",
       "  ('enquire-security_deposit', 0.0007018152),\n",
       "  ('vague-pest_control', 0.00069473404),\n",
       "  ('enquire-repayment', 0.00068458385),\n",
       "  ('enquire-alarm', 0.00060434523),\n",
       "  ('delete-delete', 0.00060418487),\n",
       "  ('enquire-meter_new', 0.00060200057),\n",
       "  ('report-payment_late', 0.0005960106),\n",
       "  ('enquire-hive_bulb', 0.00059330824),\n",
       "  ('enquire-gas_capped', 0.00057415705),\n",
       "  ('request-locksmith', 0.00054967066),\n",
       "  ('enquire-fuel_direct', 0.00053030124),\n",
       "  ('request-appointment_gas', 0.00048893393),\n",
       "  ('cancel-hive', 0.00045131106),\n",
       "  ('None', 0.0004390964)]]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\"i don't want to cancel my policy\"]\n",
    "predict_batch(model, label_list, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_pytorch_p36)",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
